{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123376"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import re\n",
    "import time\n",
    "\n",
    "with open('./spa-eng/spa.txt') as f:\n",
    "    spa_eng_list = f.read().strip().split('\\n')\n",
    "\n",
    "len(spa_eng_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>es</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>78315</th>\n",
       "      <td>Copper and silver are both metals.</td>\n",
       "      <td>El cobre y la plata son dos metales.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120197</th>\n",
       "      <td>You shouldn't say that kind of thing when chil...</td>\n",
       "      <td>No deberías decir esa clase de cosas cuando ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90698</th>\n",
       "      <td>You should be a little more tolerant.</td>\n",
       "      <td>Debería ser un poco más tolerante.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89613</th>\n",
       "      <td>Sumo is a traditional Japanese sport.</td>\n",
       "      <td>El Sumo es un deporte tradicional japonés.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86065</th>\n",
       "      <td>I will go tomorrow morning at seven.</td>\n",
       "      <td>Voy a ir mañana a las siete de la mañana.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48945</th>\n",
       "      <td>Why don't we just stay in?</td>\n",
       "      <td>¿Por qué no nos quedamos dentro?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2158</th>\n",
       "      <td>It's simple.</td>\n",
       "      <td>Es simple.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9221</th>\n",
       "      <td>Stay in the car.</td>\n",
       "      <td>¡Quedate en el auto!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54662</th>\n",
       "      <td>I have trouble with physics.</td>\n",
       "      <td>Tengo problemas con física.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111105</th>\n",
       "      <td>There's a time to speak and a time to be silent.</td>\n",
       "      <td>Hay un tiempo para hablar y un tiempo para cal...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      eng  \\\n",
       "78315                  Copper and silver are both metals.   \n",
       "120197  You shouldn't say that kind of thing when chil...   \n",
       "90698               You should be a little more tolerant.   \n",
       "89613               Sumo is a traditional Japanese sport.   \n",
       "86065                I will go tomorrow morning at seven.   \n",
       "48945                          Why don't we just stay in?   \n",
       "2158                                         It's simple.   \n",
       "9221                                     Stay in the car.   \n",
       "54662                        I have trouble with physics.   \n",
       "111105   There's a time to speak and a time to be silent.   \n",
       "\n",
       "                                                       es  \n",
       "78315                El cobre y la plata son dos metales.  \n",
       "120197  No deberías decir esa clase de cosas cuando ha...  \n",
       "90698                  Debería ser un poco más tolerante.  \n",
       "89613          El Sumo es un deporte tradicional japonés.  \n",
       "86065           Voy a ir mañana a las siete de la mañana.  \n",
       "48945                    ¿Por qué no nos quedamos dentro?  \n",
       "2158                                           Es simple.  \n",
       "9221                                 ¡Quedate en el auto!  \n",
       "54662                         Tengo problemas con física.  \n",
       "111105  Hay un tiempo para hablar y un tiempo para cal...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample size\n",
    "num_examples = 123300\n",
    "\n",
    "# creates lists containing each pair\n",
    "original_word_pairs = [[w for w in l.split('\\t')] for l in spa_eng_list[:num_examples]]\n",
    "\n",
    "data = pd.DataFrame(original_word_pairs, columns=[\"eng\", \"es\"])\n",
    "\n",
    "data.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# utils.py\n",
    "\n",
    "# Converts the unicode file to ascii\n",
    "def unicode_to_ascii(s):\n",
    "    \"\"\"\n",
    "    Normalizes latin chars with accent to their canonical decomposition\n",
    "    \"\"\"\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def preprocess_sentence(w):\n",
    "    w = unicode_to_ascii(w.lower().strip())\n",
    "    \n",
    "    # creating a space between a word and the punctuation following it\n",
    "    # eg: \"he is a boy.\" => \"he is a boy .\" \n",
    "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "    \n",
    "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "    \n",
    "    w = w.rstrip().strip()\n",
    "    \n",
    "    # adding a start and an end token to the sentence\n",
    "    # so that the model know when to start and stop predicting.\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    return w\n",
    "\n",
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)\n",
    "\n",
    "def pad_sequences(x, max_len):\n",
    "    padded = np.zeros((max_len), dtype=np.int64)\n",
    "    if len(x) > max_len:\n",
    "        padded[:] = x[:max_len]\n",
    "    else:\n",
    "        padded[:len(x)] = x\n",
    "    return padded\n",
    "\n",
    "# sort batch function to be able to use with pad_packed_sequence\n",
    "def sort_batch(X, y, lengths):\n",
    "    lengths, indx = lengths.sort(dim=0, descending=True)\n",
    "    X = X[indx]\n",
    "    y = y[indx]\n",
    "    return X.transpose(0,1), y, lengths # transpose (batch x seq) to (seq x batch)\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>es</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>122752</th>\n",
       "      <td>&lt;start&gt; tom took thousands of pictures during ...</td>\n",
       "      <td>&lt;start&gt; tom tomo miles de fotos durante sus va...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50229</th>\n",
       "      <td>&lt;start&gt; i didn t know it was there . &lt;end&gt;</td>\n",
       "      <td>&lt;start&gt; no sabia que estaba ahi . &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87084</th>\n",
       "      <td>&lt;start&gt; they were satisfied with the result . ...</td>\n",
       "      <td>&lt;start&gt; estaban satisfechos con el resultado ....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116767</th>\n",
       "      <td>&lt;start&gt; i want to wear the same kind of clothe...</td>\n",
       "      <td>&lt;start&gt; quiero usar el mismo tipo de ropa que ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94011</th>\n",
       "      <td>&lt;start&gt; i don t want to cause any more trouble...</td>\n",
       "      <td>&lt;start&gt; no quiero causar mas problemas . &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19054</th>\n",
       "      <td>&lt;start&gt; we are against war . &lt;end&gt;</td>\n",
       "      <td>&lt;start&gt; estamos en contra de la guerra . &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62709</th>\n",
       "      <td>&lt;start&gt; he is able to play the guitar . &lt;end&gt;</td>\n",
       "      <td>&lt;start&gt; sabe tocar la guitarra . &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91711</th>\n",
       "      <td>&lt;start&gt; i was about to suggest the same thing ...</td>\n",
       "      <td>&lt;start&gt; estuve a punto de sugerir lo mismo . &lt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53828</th>\n",
       "      <td>&lt;start&gt; france is in western europe . &lt;end&gt;</td>\n",
       "      <td>&lt;start&gt; francia esta en europa occidental . &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98684</th>\n",
       "      <td>&lt;start&gt; i had difficulty in solving this probl...</td>\n",
       "      <td>&lt;start&gt; tuve dificultad para resolver este pro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      eng  \\\n",
       "122752  <start> tom took thousands of pictures during ...   \n",
       "50229          <start> i didn t know it was there . <end>   \n",
       "87084   <start> they were satisfied with the result . ...   \n",
       "116767  <start> i want to wear the same kind of clothe...   \n",
       "94011   <start> i don t want to cause any more trouble...   \n",
       "19054                  <start> we are against war . <end>   \n",
       "62709       <start> he is able to play the guitar . <end>   \n",
       "91711   <start> i was about to suggest the same thing ...   \n",
       "53828         <start> france is in western europe . <end>   \n",
       "98684   <start> i had difficulty in solving this probl...   \n",
       "\n",
       "                                                       es  \n",
       "122752  <start> tom tomo miles de fotos durante sus va...  \n",
       "50229             <start> no sabia que estaba ahi . <end>  \n",
       "87084   <start> estaban satisfechos con el resultado ....  \n",
       "116767  <start> quiero usar el mismo tipo de ropa que ...  \n",
       "94011      <start> no quiero causar mas problemas . <end>  \n",
       "19054      <start> estamos en contra de la guerra . <end>  \n",
       "62709              <start> sabe tocar la guitarra . <end>  \n",
       "91711   <start> estuve a punto de sugerir lo mismo . <...  \n",
       "53828   <start> francia esta en europa occidental . <end>  \n",
       "98684   <start> tuve dificultad para resolver este pro...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# process the data\n",
    "data[\"eng\"] = data.eng.apply(lambda w: preprocess_sentence(w))\n",
    "data[\"es\"] = data.es.apply(lambda w: preprocess_sentence(w))\n",
    "data.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build the vocabulary\n",
    "class LanguageIndex():\n",
    "    \n",
    "    def __init__(self, lang):\n",
    "        \"\"\" lang: the list of phrases from each language \"\"\"\n",
    "        self.lang = lang\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.vocab = set()\n",
    "        \n",
    "        self.create_index()\n",
    "        \n",
    "    def create_index(self):\n",
    "        for phrase in self.lang:\n",
    "            # update with individual tokens\n",
    "            self.vocab.update(phrase.split(' '))\n",
    "            \n",
    "        # sort the vocab\n",
    "        self.vocab = sorted(self.vocab)\n",
    "\n",
    "        # add a padding token with index 0\n",
    "        self.word2idx['<pad>'] = 0\n",
    "        \n",
    "        # word to index mapping\n",
    "        for index, word in enumerate(self.vocab):\n",
    "            self.word2idx[word] = index + 1 # +1 because of pad token\n",
    "        \n",
    "        # index to word mapping\n",
    "        for word, index in self.word2idx.items():\n",
    "            self.idx2word[index] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5, 24440, 3, 4], [5, 24665, 3, 4], [5, 24432, 3, 4], [5, 24439, 3, 4], [5, 12839, 3, 4]]\n",
      "[[5, 5001, 3, 4], [5, 5001, 3, 4], [5, 5001, 3, 4], [5, 5001, 3, 4], [5, 5482, 3, 4]]\n",
      "max_length_inp: 38\n",
      "max_length_tar: 33\n",
      "len(input_tensor): 123300\n",
      "len(target_tensor): 123300\n",
      "len(input_tensor_train): 98640\n",
      "len(target_tensor_train): 98640\n",
      "len(input_tensor_val): 24660\n",
      "len(target_tensor_val): 24660\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# index language using the class above\n",
    "inp_lang = LanguageIndex(data[\"es\"].values.tolist())\n",
    "targ_lang = LanguageIndex(data[\"eng\"].values.tolist())\n",
    "\n",
    "# Vectorize the input and target languages\n",
    "input_tensor = [[inp_lang.word2idx[s] for s in es.split(' ')]  for es in data[\"es\"].values.tolist()]\n",
    "target_tensor = [[targ_lang.word2idx[s] for s in eng.split(' ')]  for eng in data[\"eng\"].values.tolist()]\n",
    "print(input_tensor[:5])\n",
    "print(target_tensor[:5])\n",
    "\n",
    "# calculate the max_length of input and output tensor\n",
    "max_length_inp, max_length_tar = max_length(input_tensor), max_length(target_tensor)\n",
    "print('max_length_inp: %d'%max_length_inp)\n",
    "print('max_length_tar: %d'%max_length_tar)\n",
    "\n",
    "# inplace padding\n",
    "input_tensor = [pad_sequences(x, max_length_inp) for x in input_tensor]\n",
    "target_tensor = [pad_sequences(x, max_length_tar) for x in target_tensor]\n",
    "print('len(input_tensor): %d'%len(input_tensor))\n",
    "print('len(target_tensor): %d'%len(target_tensor))\n",
    "\n",
    "# Creating training and validation sets using an 80-20 split\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
    "print('len(input_tensor_train): %d\\nlen(target_tensor_train): %d\\nlen(input_tensor_val): %d\\nlen(target_tensor_val): %d\\n'\n",
    "       % (len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13008"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(targ_lang.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# conver the data to tensors and pass to the Dataloader to create an batch iterator\n",
    "class IterData(Dataset):\n",
    "    \n",
    "    def __init__(self, X, y):\n",
    "        self.data = X\n",
    "        self.target = y\n",
    "        self.length = [ np.sum(1 - np.equal(x, 0)) for x in X]\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        y = self.target[index]\n",
    "        x_len = self.length[index]\n",
    "        return x, y, x_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hyperparams\n",
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 256\n",
    "N_BATCH = BUFFER_SIZE // BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "vocab_inp_size = len(inp_lang.word2idx)\n",
    "vocab_tar_size = len(targ_lang.word2idx)\n",
    "\n",
    "train_dataset = IterData(input_tensor_train, target_tensor_train)\n",
    "val_dataset = IterData(input_tensor_val, target_tensor_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model.py\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.enc_units = enc_units\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
    "        self.gru = nn.GRU(self.embedding_dim, self.enc_units)\n",
    "        \n",
    "    def forward(self, x, lens,  device):\n",
    "        # x: (batch_size, max_length, embedding_dim)\n",
    "        x = self.embedding(x) \n",
    "        x = pack_padded_sequence(x, lens)\n",
    "        \n",
    "        self.hidden = self.initialize_hidden_state(device)\n",
    "        \n",
    "        output, self.hidden = self.gru(x, self.hidden)\n",
    "        output, _ = pad_packed_sequence(output)\n",
    "        \n",
    "        return output, self.hidden\n",
    "        \n",
    "    def initialize_hidden_state(self, device):\n",
    "        return torch.zeros((1, self.batch_size, self.enc_units)).to(device)\n",
    "    \n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, enc_units, batch_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.dec_units = dec_units\n",
    "        self.enc_units = enc_units\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
    "        self.gru = nn.GRU(self.embedding_dim + self.enc_units,\n",
    "                                         self.dec_units,\n",
    "                                         batch_first=True)\n",
    "        self.fc = nn.Linear(self.enc_units, self.vocab_size)\n",
    "        \n",
    "        # for attention\n",
    "        self.W1 = nn.Linear(self.enc_units, self.dec_units)\n",
    "        self.W2 = nn.Linear(self.enc_units, self.dec_units)\n",
    "        self.V = nn.Linear(self.enc_units, 1)\n",
    "        \n",
    "    def forward(self, x, hidden, enc_output):\n",
    "        enc_output = enc_output.permute(1, 0, 2)\n",
    "        hidden_with_time_axis = hidden.permute(1, 0, 2)\n",
    "        score = torch.tanh(self.W1(enc_output) + self.W2(hidden_with_time_axis))\n",
    "        \n",
    "        attention_weights = torch.softmax(self.V(score), dim=1)\n",
    "        \n",
    "        context_vector = attention_weights * enc_output\n",
    "        context_vector = torch.sum(context_vector, dim=1)\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "        x = torch.cat((context_vector.unsqueeze(1), x), -1)\n",
    "        \n",
    "        output, state = self.gru(x)\n",
    "            \n",
    "        output =  output.view(-1, output.size(2))\n",
    "        \n",
    "        x = self.fc(output)\n",
    "        \n",
    "        return x, state, attention_weights\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return torch.zeros((1, self.batch_sz, self.dec_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def loss_fuction(real, pred):\n",
    "    \"\"\" Only consider non-zero inputs in the loss; mask needed \"\"\"\n",
    "    #mask = 1 - np.equal(real, 0) # assign 0 to all above 0 and 1 to all 0s\n",
    "    #print(mask)\n",
    "#    mask = real.ge(1).type(torch.FloatTensor)\n",
    "    mask = real.ge(1).type(torch.cuda.FloatTensor)\n",
    "    \n",
    "    loss_ = criterion(pred, real) * mask \n",
    "    return torch.mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Train on CUDA:1\n",
      "Encoder(\n",
      "  (embedding): Embedding(25145, 256)\n",
      "  (gru): GRU(256, 1024)\n",
      ")\n",
      "Decoder(\n",
      "  (embedding): Embedding(13009, 256)\n",
      "  (gru): GRU(1280, 1024, batch_first=True)\n",
      "  (fc): Linear(in_features=1024, out_features=13009, bias=True)\n",
      "  (W1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (W2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (V): Linear(in_features=1024, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Device\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(' Train on ' + str(device).upper())\n",
    "\n",
    "PRE_TRAINED = True\n",
    "\n",
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, units, BATCH_SIZE)\n",
    "\n",
    "if PRE_TRAINED:\n",
    "    encoder.load_state_dict(torch.load('eng2spa_encoder_params.pkl'))\n",
    "    decoder.load_state_dict(torch.load('eng2spa_decoder_params.pkl'))\n",
    "\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "print(encoder)\n",
    "print(decoder)\n",
    "\n",
    "optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train.py\n",
    "\n",
    "def train(encoder, decoder, epoch):\n",
    "    \n",
    "    dataset_train = DataLoader(train_dataset, batch_size = BATCH_SIZE, \n",
    "                     drop_last=True,\n",
    "                     shuffle=True)\n",
    "    \n",
    "    losses = AverageMeter()\n",
    "    batch_time = AverageMeter()\n",
    "\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch, (inp, targ, inp_len) in enumerate(dataset_train):\n",
    "        loss = 0\n",
    "\n",
    "        xs, ys, lens = sort_batch(inp, targ, inp_len)\n",
    "        enc_output, enc_hidden = encoder(xs.to(device), lens, device)\n",
    "        dec_hidden = enc_hidden\n",
    "        dec_input = torch.tensor([[targ_lang.word2idx['<start>']]] * BATCH_SIZE)\n",
    "        \n",
    "        end = time.time()\n",
    "\n",
    "        for t in range(1, ys.size(1)):\n",
    "            predictions, dec_hidden, _ = decoder(dec_input.to(device),\n",
    "                                                dec_hidden.to(device),\n",
    "                                                enc_output.to(device))\n",
    "            loss += loss_fuction(ys[:, t].to(device), predictions.to(device))\n",
    "            dec_input = ys[:, t].unsqueeze(1)\n",
    "\n",
    "        batch_loss = (loss / int(ys.size(1)))\n",
    "        losses.update(batch_loss, xs.size(0))\n",
    "        \n",
    "        total_loss += batch_loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if batch % PRINT_FREQ == 0:\n",
    "                print('Epoch: [{0}] [{1}/{2}]\\t'\n",
    "                      'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                      .format(\n",
    "                       epoch, batch, len(dataset_train), batch_time=batch_time, loss=losses))\n",
    "                \n",
    "        if batch % 200 == 0:\n",
    "            print('Saving Model...')\n",
    "            torch.save(encoder.state_dict(), 'eng2spa_encoder_params.pkl')\n",
    "            torch.save(decoder.state_dict(), 'eng2spa_decoder_params.pkl')\n",
    "            print('Model Saved Successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validate(encoder, decoder):\n",
    "    \n",
    "    dataset_val = DataLoader(val_dataset, batch_size = 1, \n",
    "                     drop_last=True,\n",
    "                     shuffle=False)\n",
    "    \n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    acc = 0\n",
    "    for batch, (inp, targ, inp_len) in enumerate(dataset_val):\n",
    "        xs, ys, lens = sort_batch(inp, targ, inp_len)\n",
    "        raw_sen = xs.numpy()\n",
    "#         print(raw_sen.shape)\n",
    "        for raw_word in raw_sen:\n",
    "#             print(raw_word)\n",
    "            print(inp_lang.idx2word[int(raw_word)], end=' ')\n",
    "        print()\n",
    "        raw_sen1 = ys.numpy()[0]\n",
    "#         print(raw_sen1.shape)\n",
    "        for raw_word1 in raw_sen1:\n",
    "#             print(raw_word)\n",
    "            print(targ_lang.idx2word[int(raw_word1)], end=' ')\n",
    "        print()\n",
    "        input()\n",
    "        enc_output, enc_hidden = encoder(xs.to(device), lens, device)\n",
    "        dec_hidden = enc_hidden\n",
    "        dec_input = torch.tensor([[targ_lang.word2idx['<start>']]] * 1)\n",
    "        for t in range(1, ys.size(1)):\n",
    "#             print(ys.size(1))\n",
    "            predictions, dec_hidden, _ = decoder(dec_input.to(device),\n",
    "                                        dec_hidden.to(device),\n",
    "                                        enc_output.to(device))\n",
    "#             print(predictions.shape)\n",
    "#             print(ys[:,t])\n",
    "            pred = torch.argmax(predictions)\n",
    "#             print(predictions.detach().numpy())\n",
    "#             print(predictions.detach().numpy()[0][3])\n",
    "#             print(int(pred.numpy()))\n",
    "            print(targ_lang.idx2word[int(pred.cpu().numpy())], end=' ')\n",
    "        print()\n",
    "#             input()\n",
    "#         print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def apply(encoder, decoder):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    x = input('Input an Spanish sentence to translate: ')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0] [0/385]\tTime 0.428 (0.428)\tLoss 2.4471 (2.4471)\t\n",
      "Saving Model...\n",
      "Model Saved Successfully!\n",
      "Epoch: [0] [5/385]\tTime 0.350 (0.376)\tLoss 1.5620 (2.0568)\t\n",
      "Epoch: [0] [10/385]\tTime 0.355 (0.382)\tLoss 1.2576 (1.7140)\t\n",
      "Epoch: [0] [15/385]\tTime 0.351 (0.385)\tLoss 1.2350 (1.5664)\t\n",
      "Epoch: [0] [20/385]\tTime 0.404 (0.388)\tLoss 1.1673 (1.4713)\t\n",
      "Epoch: [0] [25/385]\tTime 0.465 (0.392)\tLoss 1.0750 (1.4062)\t\n",
      "Epoch: [0] [30/385]\tTime 0.404 (0.394)\tLoss 1.1318 (1.3585)\t\n",
      "Epoch: [0] [35/385]\tTime 0.356 (0.390)\tLoss 1.0679 (1.3211)\t\n",
      "Epoch: [0] [40/385]\tTime 0.400 (0.390)\tLoss 1.0220 (1.2886)\t\n",
      "Epoch: [0] [45/385]\tTime 0.411 (0.394)\tLoss 1.0551 (1.2603)\t\n",
      "Epoch: [0] [50/385]\tTime 0.479 (0.396)\tLoss 1.0399 (1.2381)\t\n",
      "Epoch: [0] [55/385]\tTime 0.383 (0.397)\tLoss 0.9931 (1.2148)\t\n",
      "Epoch: [0] [60/385]\tTime 0.418 (0.395)\tLoss 0.9583 (1.1947)\t\n",
      "Epoch: [0] [65/385]\tTime 0.433 (0.395)\tLoss 0.9830 (1.1757)\t\n",
      "Epoch: [0] [70/385]\tTime 0.401 (0.396)\tLoss 0.8860 (1.1589)\t\n",
      "Epoch: [0] [75/385]\tTime 0.410 (0.398)\tLoss 0.9074 (1.1427)\t\n",
      "Epoch: [0] [80/385]\tTime 0.438 (0.398)\tLoss 0.9123 (1.1270)\t\n",
      "Epoch: [0] [85/385]\tTime 0.377 (0.399)\tLoss 0.8126 (1.1108)\t\n",
      "Epoch: [0] [90/385]\tTime 0.373 (0.399)\tLoss 0.8720 (1.0974)\t\n",
      "Epoch: [0] [95/385]\tTime 0.399 (0.398)\tLoss 0.8279 (1.0845)\t\n",
      "Epoch: [0] [100/385]\tTime 0.393 (0.398)\tLoss 0.7955 (1.0711)\t\n",
      "Epoch: [0] [105/385]\tTime 0.391 (0.399)\tLoss 0.7912 (1.0587)\t\n",
      "Epoch: [0] [110/385]\tTime 0.446 (0.400)\tLoss 0.7745 (1.0466)\t\n",
      "Epoch: [0] [115/385]\tTime 0.392 (0.400)\tLoss 0.7984 (1.0354)\t\n",
      "Epoch: [0] [120/385]\tTime 0.382 (0.399)\tLoss 0.7660 (1.0248)\t\n",
      "Epoch: [0] [125/385]\tTime 0.411 (0.399)\tLoss 0.7669 (1.0139)\t\n",
      "Epoch: [0] [130/385]\tTime 0.362 (0.399)\tLoss 0.7537 (1.0037)\t\n",
      "Epoch: [0] [135/385]\tTime 0.361 (0.398)\tLoss 0.7388 (0.9945)\t\n",
      "Epoch: [0] [140/385]\tTime 0.415 (0.398)\tLoss 0.7280 (0.9853)\t\n",
      "Epoch: [0] [145/385]\tTime 0.454 (0.400)\tLoss 0.7291 (0.9765)\t\n",
      "Epoch: [0] [150/385]\tTime 0.387 (0.400)\tLoss 0.6984 (0.9673)\t\n",
      "Epoch: [0] [155/385]\tTime 0.377 (0.399)\tLoss 0.7034 (0.9591)\t\n",
      "Epoch: [0] [160/385]\tTime 0.402 (0.399)\tLoss 0.6565 (0.9509)\t\n",
      "Epoch: [0] [165/385]\tTime 0.355 (0.399)\tLoss 0.6731 (0.9429)\t\n",
      "Epoch: [0] [170/385]\tTime 0.404 (0.398)\tLoss 0.6974 (0.9351)\t\n",
      "Epoch: [0] [175/385]\tTime 0.428 (0.399)\tLoss 0.6637 (0.9277)\t\n",
      "Epoch: [0] [180/385]\tTime 0.397 (0.400)\tLoss 0.6704 (0.9200)\t\n",
      "Epoch: [0] [185/385]\tTime 0.453 (0.401)\tLoss 0.6914 (0.9130)\t\n",
      "Epoch: [0] [190/385]\tTime 0.388 (0.400)\tLoss 0.6122 (0.9053)\t\n",
      "Epoch: [0] [195/385]\tTime 0.370 (0.400)\tLoss 0.6428 (0.8987)\t\n",
      "Epoch: [0] [200/385]\tTime 0.352 (0.400)\tLoss 0.6130 (0.8921)\t\n",
      "Saving Model...\n",
      "Model Saved Successfully!\n",
      "Epoch: [0] [205/385]\tTime 0.444 (0.400)\tLoss 0.6060 (0.8857)\t\n",
      "Epoch: [0] [210/385]\tTime 0.435 (0.400)\tLoss 0.6522 (0.8795)\t\n",
      "Epoch: [0] [215/385]\tTime 0.364 (0.400)\tLoss 0.5954 (0.8736)\t\n",
      "Epoch: [0] [220/385]\tTime 0.389 (0.400)\tLoss 0.6295 (0.8674)\t\n",
      "Epoch: [0] [225/385]\tTime 0.378 (0.400)\tLoss 0.5618 (0.8611)\t\n",
      "Epoch: [0] [230/385]\tTime 0.414 (0.400)\tLoss 0.5999 (0.8554)\t\n",
      "Epoch: [0] [235/385]\tTime 0.381 (0.399)\tLoss 0.5483 (0.8493)\t\n",
      "Epoch: [0] [240/385]\tTime 0.406 (0.399)\tLoss 0.5782 (0.8433)\t\n",
      "Epoch: [0] [245/385]\tTime 0.412 (0.400)\tLoss 0.5391 (0.8376)\t\n",
      "Epoch: [0] [250/385]\tTime 0.433 (0.400)\tLoss 0.5827 (0.8324)\t\n",
      "Epoch: [0] [255/385]\tTime 0.400 (0.401)\tLoss 0.5399 (0.8268)\t\n",
      "Epoch: [0] [260/385]\tTime 0.415 (0.400)\tLoss 0.5840 (0.8216)\t\n",
      "Epoch: [0] [265/385]\tTime 0.415 (0.401)\tLoss 0.5794 (0.8164)\t\n",
      "Epoch: [0] [270/385]\tTime 0.390 (0.400)\tLoss 0.4867 (0.8111)\t\n",
      "Epoch: [0] [275/385]\tTime 0.434 (0.400)\tLoss 0.5527 (0.8059)\t\n",
      "Epoch: [0] [280/385]\tTime 0.372 (0.400)\tLoss 0.5200 (0.8010)\t\n",
      "Epoch: [0] [285/385]\tTime 0.404 (0.399)\tLoss 0.5657 (0.7964)\t\n",
      "Epoch: [0] [290/385]\tTime 0.403 (0.400)\tLoss 0.5298 (0.7918)\t\n",
      "Epoch: [0] [295/385]\tTime 0.457 (0.401)\tLoss 0.5255 (0.7871)\t\n",
      "Epoch: [0] [300/385]\tTime 0.376 (0.401)\tLoss 0.5267 (0.7824)\t\n",
      "Epoch: [0] [305/385]\tTime 0.377 (0.400)\tLoss 0.5119 (0.7780)\t\n",
      "Epoch: [0] [310/385]\tTime 0.400 (0.401)\tLoss 0.4780 (0.7736)\t\n",
      "Epoch: [0] [315/385]\tTime 0.449 (0.401)\tLoss 0.4968 (0.7692)\t\n",
      "Epoch: [0] [320/385]\tTime 0.372 (0.401)\tLoss 0.4918 (0.7648)\t\n",
      "Epoch: [0] [325/385]\tTime 0.421 (0.401)\tLoss 0.5365 (0.7610)\t\n",
      "Epoch: [0] [330/385]\tTime 0.389 (0.400)\tLoss 0.4805 (0.7567)\t\n",
      "Epoch: [0] [335/385]\tTime 0.388 (0.401)\tLoss 0.4933 (0.7526)\t\n",
      "Epoch: [0] [340/385]\tTime 0.494 (0.401)\tLoss 0.5014 (0.7488)\t\n",
      "Epoch: [0] [345/385]\tTime 0.480 (0.402)\tLoss 0.4905 (0.7450)\t\n",
      "Epoch: [0] [350/385]\tTime 0.383 (0.401)\tLoss 0.4532 (0.7411)\t\n",
      "Epoch: [0] [355/385]\tTime 0.412 (0.401)\tLoss 0.5069 (0.7377)\t\n",
      "Epoch: [0] [360/385]\tTime 0.388 (0.402)\tLoss 0.4723 (0.7343)\t\n",
      "Epoch: [0] [365/385]\tTime 0.378 (0.401)\tLoss 0.4561 (0.7306)\t\n",
      "Epoch: [0] [370/385]\tTime 0.398 (0.401)\tLoss 0.4935 (0.7271)\t\n",
      "Epoch: [0] [375/385]\tTime 0.396 (0.401)\tLoss 0.4284 (0.7235)\t\n",
      "Epoch: [0] [380/385]\tTime 0.376 (0.401)\tLoss 0.4458 (0.7200)\t\n",
      "Saving Model...\n",
      "Model Saved Successfully!\n",
      "Epoch: [1] [0/385]\tTime 0.432 (0.432)\tLoss 0.3927 (0.3927)\t\n",
      "Saving Model...\n",
      "Model Saved Successfully!\n",
      "Epoch: [1] [5/385]\tTime 0.426 (0.422)\tLoss 0.3986 (0.3867)\t\n",
      "Epoch: [1] [10/385]\tTime 0.462 (0.422)\tLoss 0.3808 (0.3861)\t\n",
      "Epoch: [1] [15/385]\tTime 0.373 (0.408)\tLoss 0.3999 (0.3879)\t\n",
      "Epoch: [1] [20/385]\tTime 0.409 (0.405)\tLoss 0.3911 (0.3868)\t\n",
      "Epoch: [1] [25/385]\tTime 0.413 (0.410)\tLoss 0.3818 (0.3854)\t\n",
      "Epoch: [1] [30/385]\tTime 0.364 (0.409)\tLoss 0.3817 (0.3874)\t\n",
      "Epoch: [1] [35/385]\tTime 0.481 (0.409)\tLoss 0.3958 (0.3871)\t\n",
      "Epoch: [1] [40/385]\tTime 0.363 (0.405)\tLoss 0.3844 (0.3860)\t\n",
      "Epoch: [1] [45/385]\tTime 0.373 (0.406)\tLoss 0.3543 (0.3855)\t\n",
      "Epoch: [1] [50/385]\tTime 0.395 (0.404)\tLoss 0.3915 (0.3856)\t\n",
      "Epoch: [1] [55/385]\tTime 0.392 (0.405)\tLoss 0.3692 (0.3843)\t\n",
      "Epoch: [1] [60/385]\tTime 0.387 (0.404)\tLoss 0.3873 (0.3840)\t\n",
      "Epoch: [1] [65/385]\tTime 0.403 (0.404)\tLoss 0.3930 (0.3834)\t\n",
      "Epoch: [1] [70/385]\tTime 0.435 (0.405)\tLoss 0.3716 (0.3835)\t\n",
      "Epoch: [1] [75/385]\tTime 0.541 (0.407)\tLoss 0.3558 (0.3831)\t\n",
      "Epoch: [1] [80/385]\tTime 0.395 (0.407)\tLoss 0.3891 (0.3828)\t\n",
      "Epoch: [1] [85/385]\tTime 0.439 (0.406)\tLoss 0.3705 (0.3827)\t\n",
      "Epoch: [1] [90/385]\tTime 0.368 (0.405)\tLoss 0.3836 (0.3825)\t\n",
      "Epoch: [1] [95/385]\tTime 0.387 (0.404)\tLoss 0.4080 (0.3820)\t\n",
      "Epoch: [1] [100/385]\tTime 0.462 (0.404)\tLoss 0.3867 (0.3816)\t\n",
      "Epoch: [1] [105/385]\tTime 0.377 (0.404)\tLoss 0.3416 (0.3802)\t\n",
      "Epoch: [1] [110/385]\tTime 0.362 (0.403)\tLoss 0.3589 (0.3794)\t\n",
      "Epoch: [1] [115/385]\tTime 0.428 (0.403)\tLoss 0.3590 (0.3791)\t\n",
      "Epoch: [1] [120/385]\tTime 0.387 (0.403)\tLoss 0.3485 (0.3780)\t\n",
      "Epoch: [1] [125/385]\tTime 0.385 (0.404)\tLoss 0.3766 (0.3775)\t\n",
      "Epoch: [1] [130/385]\tTime 0.361 (0.403)\tLoss 0.3869 (0.3775)\t\n",
      "Epoch: [1] [135/385]\tTime 0.455 (0.403)\tLoss 0.3664 (0.3770)\t\n",
      "Epoch: [1] [140/385]\tTime 0.394 (0.403)\tLoss 0.3534 (0.3763)\t\n",
      "Epoch: [1] [145/385]\tTime 0.421 (0.403)\tLoss 0.3755 (0.3757)\t\n",
      "Epoch: [1] [150/385]\tTime 0.422 (0.403)\tLoss 0.3569 (0.3752)\t\n",
      "Epoch: [1] [155/385]\tTime 0.369 (0.402)\tLoss 0.3418 (0.3746)\t\n",
      "Epoch: [1] [160/385]\tTime 0.416 (0.403)\tLoss 0.3505 (0.3741)\t\n",
      "Epoch: [1] [165/385]\tTime 0.357 (0.402)\tLoss 0.3261 (0.3733)\t\n",
      "Epoch: [1] [170/385]\tTime 0.396 (0.402)\tLoss 0.3546 (0.3729)\t\n",
      "Epoch: [1] [175/385]\tTime 0.454 (0.402)\tLoss 0.3287 (0.3721)\t\n",
      "Epoch: [1] [180/385]\tTime 0.411 (0.402)\tLoss 0.3654 (0.3717)\t\n",
      "Epoch: [1] [185/385]\tTime 0.420 (0.403)\tLoss 0.2818 (0.3706)\t\n",
      "Epoch: [1] [190/385]\tTime 0.414 (0.403)\tLoss 0.3618 (0.3701)\t\n",
      "Epoch: [1] [195/385]\tTime 0.360 (0.402)\tLoss 0.3242 (0.3695)\t\n",
      "Epoch: [1] [200/385]\tTime 0.411 (0.403)\tLoss 0.3547 (0.3689)\t\n",
      "Saving Model...\n",
      "Model Saved Successfully!\n",
      "Epoch: [1] [205/385]\tTime 0.416 (0.403)\tLoss 0.3453 (0.3684)\t\n",
      "Epoch: [1] [210/385]\tTime 0.363 (0.403)\tLoss 0.3508 (0.3680)\t\n",
      "Epoch: [1] [215/385]\tTime 0.411 (0.403)\tLoss 0.3283 (0.3673)\t\n",
      "Epoch: [1] [220/385]\tTime 0.454 (0.404)\tLoss 0.3227 (0.3668)\t\n",
      "Epoch: [1] [225/385]\tTime 0.368 (0.403)\tLoss 0.3395 (0.3660)\t\n",
      "Epoch: [1] [230/385]\tTime 0.404 (0.403)\tLoss 0.3519 (0.3655)\t\n",
      "Epoch: [1] [235/385]\tTime 0.381 (0.403)\tLoss 0.3718 (0.3650)\t\n",
      "Epoch: [1] [240/385]\tTime 0.406 (0.403)\tLoss 0.3398 (0.3642)\t\n",
      "Epoch: [1] [245/385]\tTime 0.394 (0.403)\tLoss 0.3037 (0.3635)\t\n",
      "Epoch: [1] [250/385]\tTime 0.405 (0.402)\tLoss 0.3210 (0.3632)\t\n",
      "Epoch: [1] [255/385]\tTime 0.380 (0.402)\tLoss 0.3219 (0.3624)\t\n",
      "Epoch: [1] [260/385]\tTime 0.392 (0.402)\tLoss 0.3348 (0.3617)\t\n",
      "Epoch: [1] [265/385]\tTime 0.371 (0.402)\tLoss 0.3335 (0.3610)\t\n",
      "Epoch: [1] [270/385]\tTime 0.403 (0.402)\tLoss 0.3591 (0.3605)\t\n",
      "Epoch: [1] [275/385]\tTime 0.402 (0.401)\tLoss 0.3431 (0.3599)\t\n",
      "Epoch: [1] [280/385]\tTime 0.398 (0.401)\tLoss 0.3421 (0.3595)\t\n",
      "Epoch: [1] [285/385]\tTime 0.400 (0.402)\tLoss 0.3321 (0.3589)\t\n",
      "Epoch: [1] [290/385]\tTime 0.396 (0.402)\tLoss 0.3524 (0.3585)\t\n",
      "Epoch: [1] [295/385]\tTime 0.428 (0.402)\tLoss 0.3120 (0.3579)\t\n",
      "Epoch: [1] [300/385]\tTime 0.474 (0.403)\tLoss 0.3404 (0.3574)\t\n",
      "Epoch: [1] [305/385]\tTime 0.408 (0.403)\tLoss 0.3258 (0.3569)\t\n",
      "Epoch: [1] [310/385]\tTime 0.389 (0.403)\tLoss 0.3482 (0.3563)\t\n",
      "Epoch: [1] [315/385]\tTime 0.445 (0.404)\tLoss 0.3427 (0.3556)\t\n",
      "Epoch: [1] [320/385]\tTime 0.429 (0.404)\tLoss 0.3506 (0.3551)\t\n",
      "Epoch: [1] [325/385]\tTime 0.418 (0.404)\tLoss 0.3284 (0.3547)\t\n",
      "Epoch: [1] [330/385]\tTime 0.396 (0.403)\tLoss 0.2905 (0.3542)\t\n",
      "Epoch: [1] [335/385]\tTime 0.392 (0.404)\tLoss 0.3286 (0.3538)\t\n",
      "Epoch: [1] [340/385]\tTime 0.378 (0.403)\tLoss 0.3095 (0.3533)\t\n",
      "Epoch: [1] [345/385]\tTime 0.377 (0.403)\tLoss 0.3341 (0.3528)\t\n",
      "Epoch: [1] [350/385]\tTime 0.409 (0.403)\tLoss 0.2957 (0.3524)\t\n",
      "Epoch: [1] [355/385]\tTime 0.373 (0.403)\tLoss 0.3481 (0.3521)\t\n",
      "Epoch: [1] [360/385]\tTime 0.440 (0.403)\tLoss 0.3765 (0.3517)\t\n",
      "Epoch: [1] [365/385]\tTime 0.376 (0.403)\tLoss 0.3218 (0.3514)\t\n",
      "Epoch: [1] [370/385]\tTime 0.380 (0.403)\tLoss 0.3163 (0.3510)\t\n",
      "Epoch: [1] [375/385]\tTime 0.434 (0.403)\tLoss 0.3411 (0.3508)\t\n",
      "Epoch: [1] [380/385]\tTime 0.509 (0.403)\tLoss 0.3194 (0.3501)\t\n",
      "Saving Model...\n",
      "Model Saved Successfully!\n",
      "Epoch: [2] [0/385]\tTime 0.443 (0.443)\tLoss 0.2304 (0.2304)\t\n",
      "Saving Model...\n",
      "Model Saved Successfully!\n",
      "Epoch: [2] [5/385]\tTime 0.409 (0.413)\tLoss 0.2393 (0.2361)\t\n",
      "Epoch: [2] [10/385]\tTime 0.390 (0.397)\tLoss 0.2450 (0.2391)\t\n",
      "Epoch: [2] [15/385]\tTime 0.373 (0.402)\tLoss 0.2150 (0.2387)\t\n",
      "Epoch: [2] [20/385]\tTime 0.386 (0.397)\tLoss 0.2042 (0.2330)\t\n",
      "Epoch: [2] [25/385]\tTime 0.409 (0.395)\tLoss 0.2348 (0.2319)\t\n",
      "Epoch: [2] [30/385]\tTime 0.452 (0.397)\tLoss 0.2396 (0.2326)\t\n",
      "Epoch: [2] [35/385]\tTime 0.359 (0.396)\tLoss 0.2383 (0.2321)\t\n",
      "Epoch: [2] [40/385]\tTime 0.411 (0.397)\tLoss 0.2246 (0.2312)\t\n",
      "Epoch: [2] [45/385]\tTime 0.403 (0.396)\tLoss 0.2484 (0.2307)\t\n",
      "Epoch: [2] [50/385]\tTime 0.428 (0.400)\tLoss 0.2397 (0.2322)\t\n",
      "Epoch: [2] [55/385]\tTime 0.390 (0.400)\tLoss 0.2312 (0.2340)\t\n",
      "Epoch: [2] [60/385]\tTime 0.456 (0.404)\tLoss 0.2359 (0.2338)\t\n",
      "Epoch: [2] [65/385]\tTime 0.437 (0.405)\tLoss 0.2197 (0.2333)\t\n",
      "Epoch: [2] [70/385]\tTime 0.379 (0.404)\tLoss 0.2339 (0.2333)\t\n",
      "Epoch: [2] [75/385]\tTime 0.419 (0.404)\tLoss 0.2341 (0.2331)\t\n",
      "Epoch: [2] [80/385]\tTime 0.426 (0.406)\tLoss 0.2541 (0.2335)\t\n",
      "Epoch: [2] [85/385]\tTime 0.513 (0.408)\tLoss 0.2266 (0.2328)\t\n",
      "Epoch: [2] [90/385]\tTime 0.427 (0.410)\tLoss 0.2490 (0.2340)\t\n",
      "Epoch: [2] [95/385]\tTime 0.382 (0.408)\tLoss 0.2134 (0.2337)\t\n",
      "Epoch: [2] [100/385]\tTime 0.400 (0.409)\tLoss 0.2299 (0.2336)\t\n",
      "Epoch: [2] [105/385]\tTime 0.393 (0.408)\tLoss 0.2517 (0.2338)\t\n",
      "Epoch: [2] [110/385]\tTime 0.362 (0.407)\tLoss 0.2273 (0.2334)\t\n",
      "Epoch: [2] [115/385]\tTime 0.374 (0.406)\tLoss 0.2176 (0.2330)\t\n",
      "Epoch: [2] [120/385]\tTime 0.433 (0.406)\tLoss 0.2098 (0.2331)\t\n",
      "Epoch: [2] [125/385]\tTime 0.358 (0.405)\tLoss 0.2430 (0.2333)\t\n",
      "Epoch: [2] [130/385]\tTime 0.433 (0.405)\tLoss 0.2637 (0.2329)\t\n",
      "Epoch: [2] [135/385]\tTime 0.428 (0.405)\tLoss 0.2244 (0.2332)\t\n",
      "Epoch: [2] [140/385]\tTime 0.421 (0.406)\tLoss 0.2246 (0.2329)\t\n",
      "Epoch: [2] [145/385]\tTime 0.369 (0.405)\tLoss 0.2319 (0.2331)\t\n",
      "Epoch: [2] [150/385]\tTime 0.373 (0.404)\tLoss 0.2537 (0.2332)\t\n",
      "Epoch: [2] [155/385]\tTime 0.432 (0.405)\tLoss 0.2509 (0.2333)\t\n",
      "Epoch: [2] [160/385]\tTime 0.420 (0.406)\tLoss 0.2466 (0.2337)\t\n",
      "Epoch: [2] [165/385]\tTime 0.406 (0.406)\tLoss 0.2086 (0.2334)\t\n",
      "Epoch: [2] [170/385]\tTime 0.380 (0.405)\tLoss 0.2416 (0.2337)\t\n",
      "Epoch: [2] [175/385]\tTime 0.402 (0.405)\tLoss 0.2294 (0.2334)\t\n",
      "Epoch: [2] [180/385]\tTime 0.387 (0.405)\tLoss 0.2466 (0.2337)\t\n",
      "Epoch: [2] [185/385]\tTime 0.426 (0.405)\tLoss 0.2209 (0.2337)\t\n",
      "Epoch: [2] [190/385]\tTime 0.371 (0.405)\tLoss 0.2607 (0.2339)\t\n",
      "Epoch: [2] [195/385]\tTime 0.376 (0.404)\tLoss 0.2489 (0.2342)\t\n",
      "Epoch: [2] [200/385]\tTime 0.439 (0.405)\tLoss 0.2263 (0.2343)\t\n",
      "Saving Model...\n",
      "Model Saved Successfully!\n",
      "Epoch: [2] [205/385]\tTime 0.457 (0.405)\tLoss 0.2377 (0.2345)\t\n",
      "Epoch: [2] [210/385]\tTime 0.395 (0.404)\tLoss 0.2437 (0.2342)\t\n",
      "Epoch: [2] [215/385]\tTime 0.411 (0.404)\tLoss 0.2447 (0.2342)\t\n",
      "Epoch: [2] [220/385]\tTime 0.382 (0.404)\tLoss 0.2617 (0.2342)\t\n",
      "Epoch: [2] [225/385]\tTime 0.419 (0.404)\tLoss 0.2435 (0.2344)\t\n",
      "Epoch: [2] [230/385]\tTime 0.370 (0.403)\tLoss 0.2097 (0.2342)\t\n",
      "Epoch: [2] [235/385]\tTime 0.428 (0.403)\tLoss 0.2393 (0.2340)\t\n",
      "Epoch: [2] [240/385]\tTime 0.429 (0.403)\tLoss 0.2415 (0.2341)\t\n",
      "Epoch: [2] [245/385]\tTime 0.458 (0.403)\tLoss 0.2482 (0.2344)\t\n",
      "Epoch: [2] [250/385]\tTime 0.405 (0.404)\tLoss 0.2587 (0.2343)\t\n",
      "Epoch: [2] [255/385]\tTime 0.360 (0.403)\tLoss 0.2270 (0.2342)\t\n",
      "Epoch: [2] [260/385]\tTime 0.430 (0.403)\tLoss 0.2402 (0.2342)\t\n",
      "Epoch: [2] [265/385]\tTime 0.419 (0.403)\tLoss 0.2437 (0.2342)\t\n",
      "Epoch: [2] [270/385]\tTime 0.411 (0.404)\tLoss 0.2136 (0.2342)\t\n",
      "Epoch: [2] [275/385]\tTime 0.383 (0.404)\tLoss 0.2550 (0.2343)\t\n",
      "Epoch: [2] [280/385]\tTime 0.479 (0.404)\tLoss 0.2519 (0.2344)\t\n",
      "Epoch: [2] [285/385]\tTime 0.355 (0.404)\tLoss 0.2528 (0.2342)\t\n",
      "Epoch: [2] [290/385]\tTime 0.396 (0.404)\tLoss 0.2140 (0.2342)\t\n",
      "Epoch: [2] [295/385]\tTime 0.401 (0.404)\tLoss 0.2226 (0.2342)\t\n",
      "Epoch: [2] [300/385]\tTime 0.427 (0.404)\tLoss 0.2287 (0.2341)\t\n",
      "Epoch: [2] [305/385]\tTime 0.359 (0.403)\tLoss 0.1998 (0.2338)\t\n",
      "Epoch: [2] [310/385]\tTime 0.388 (0.403)\tLoss 0.2167 (0.2337)\t\n",
      "Epoch: [2] [315/385]\tTime 0.367 (0.403)\tLoss 0.2332 (0.2335)\t\n",
      "Epoch: [2] [320/385]\tTime 0.447 (0.403)\tLoss 0.2406 (0.2336)\t\n",
      "Epoch: [2] [325/385]\tTime 0.397 (0.403)\tLoss 0.2516 (0.2338)\t\n",
      "Epoch: [2] [330/385]\tTime 0.485 (0.404)\tLoss 0.2299 (0.2338)\t\n",
      "Epoch: [2] [335/385]\tTime 0.379 (0.403)\tLoss 0.2277 (0.2335)\t\n",
      "Epoch: [2] [340/385]\tTime 0.459 (0.404)\tLoss 0.2237 (0.2335)\t\n",
      "Epoch: [2] [345/385]\tTime 0.426 (0.404)\tLoss 0.2422 (0.2335)\t\n",
      "Epoch: [2] [350/385]\tTime 0.395 (0.404)\tLoss 0.2089 (0.2334)\t\n",
      "Epoch: [2] [355/385]\tTime 0.433 (0.404)\tLoss 0.2467 (0.2333)\t\n",
      "Epoch: [2] [360/385]\tTime 0.388 (0.404)\tLoss 0.2419 (0.2333)\t\n",
      "Epoch: [2] [365/385]\tTime 0.342 (0.404)\tLoss 0.2252 (0.2333)\t\n",
      "Epoch: [2] [370/385]\tTime 0.433 (0.404)\tLoss 0.2364 (0.2333)\t\n",
      "Epoch: [2] [375/385]\tTime 0.529 (0.405)\tLoss 0.2342 (0.2331)\t\n",
      "Epoch: [2] [380/385]\tTime 0.439 (0.405)\tLoss 0.2110 (0.2330)\t\n",
      "Saving Model...\n",
      "Model Saved Successfully!\n",
      "Epoch: [3] [0/385]\tTime 0.441 (0.441)\tLoss 0.1622 (0.1622)\t\n",
      "Saving Model...\n",
      "Model Saved Successfully!\n",
      "Epoch: [3] [5/385]\tTime 0.388 (0.422)\tLoss 0.1556 (0.1662)\t\n",
      "Epoch: [3] [10/385]\tTime 0.367 (0.415)\tLoss 0.1479 (0.1599)\t\n",
      "Epoch: [3] [15/385]\tTime 0.411 (0.409)\tLoss 0.1565 (0.1584)\t\n",
      "Epoch: [3] [20/385]\tTime 0.357 (0.401)\tLoss 0.1558 (0.1584)\t\n",
      "Epoch: [3] [25/385]\tTime 0.386 (0.403)\tLoss 0.1494 (0.1558)\t\n",
      "Epoch: [3] [30/385]\tTime 0.370 (0.399)\tLoss 0.1707 (0.1574)\t\n",
      "Epoch: [3] [35/385]\tTime 0.397 (0.402)\tLoss 0.1612 (0.1580)\t\n",
      "Epoch: [3] [40/385]\tTime 0.387 (0.400)\tLoss 0.1577 (0.1580)\t\n",
      "Epoch: [3] [45/385]\tTime 0.374 (0.401)\tLoss 0.1526 (0.1583)\t\n",
      "Epoch: [3] [50/385]\tTime 0.359 (0.402)\tLoss 0.1561 (0.1588)\t\n",
      "Epoch: [3] [55/385]\tTime 0.407 (0.400)\tLoss 0.1554 (0.1590)\t\n",
      "Epoch: [3] [60/385]\tTime 0.376 (0.401)\tLoss 0.1722 (0.1598)\t\n",
      "Epoch: [3] [65/385]\tTime 0.387 (0.399)\tLoss 0.1591 (0.1602)\t\n",
      "Epoch: [3] [70/385]\tTime 0.431 (0.400)\tLoss 0.1885 (0.1607)\t\n",
      "Epoch: [3] [75/385]\tTime 0.390 (0.398)\tLoss 0.1346 (0.1603)\t\n",
      "Epoch: [3] [80/385]\tTime 0.384 (0.398)\tLoss 0.1591 (0.1608)\t\n",
      "Epoch: [3] [85/385]\tTime 0.431 (0.399)\tLoss 0.1536 (0.1610)\t\n",
      "Epoch: [3] [90/385]\tTime 0.414 (0.399)\tLoss 0.1867 (0.1619)\t\n",
      "Epoch: [3] [95/385]\tTime 0.417 (0.402)\tLoss 0.1619 (0.1620)\t\n",
      "Epoch: [3] [100/385]\tTime 0.369 (0.401)\tLoss 0.1556 (0.1618)\t\n",
      "Epoch: [3] [105/385]\tTime 0.439 (0.401)\tLoss 0.1689 (0.1621)\t\n",
      "Epoch: [3] [110/385]\tTime 0.378 (0.401)\tLoss 0.1657 (0.1622)\t\n",
      "Epoch: [3] [115/385]\tTime 0.352 (0.401)\tLoss 0.1707 (0.1624)\t\n",
      "Epoch: [3] [120/385]\tTime 0.417 (0.402)\tLoss 0.1623 (0.1623)\t\n",
      "Epoch: [3] [125/385]\tTime 0.394 (0.403)\tLoss 0.1495 (0.1622)\t\n",
      "Epoch: [3] [130/385]\tTime 0.389 (0.403)\tLoss 0.1588 (0.1623)\t\n",
      "Epoch: [3] [135/385]\tTime 0.411 (0.403)\tLoss 0.1650 (0.1622)\t\n",
      "Epoch: [3] [140/385]\tTime 0.402 (0.403)\tLoss 0.1719 (0.1623)\t\n",
      "Epoch: [3] [145/385]\tTime 0.406 (0.403)\tLoss 0.1535 (0.1623)\t\n",
      "Epoch: [3] [150/385]\tTime 0.389 (0.402)\tLoss 0.1488 (0.1621)\t\n",
      "Epoch: [3] [155/385]\tTime 0.382 (0.402)\tLoss 0.1799 (0.1621)\t\n",
      "Epoch: [3] [160/385]\tTime 0.456 (0.403)\tLoss 0.1637 (0.1620)\t\n",
      "Epoch: [3] [165/385]\tTime 0.410 (0.404)\tLoss 0.1658 (0.1622)\t\n",
      "Epoch: [3] [170/385]\tTime 0.370 (0.403)\tLoss 0.1670 (0.1623)\t\n",
      "Epoch: [3] [175/385]\tTime 0.385 (0.403)\tLoss 0.1590 (0.1625)\t\n",
      "Epoch: [3] [180/385]\tTime 0.397 (0.403)\tLoss 0.1588 (0.1626)\t\n",
      "Epoch: [3] [185/385]\tTime 0.399 (0.403)\tLoss 0.1658 (0.1627)\t\n",
      "Epoch: [3] [190/385]\tTime 0.401 (0.403)\tLoss 0.1677 (0.1627)\t\n",
      "Epoch: [3] [195/385]\tTime 0.409 (0.403)\tLoss 0.1570 (0.1626)\t\n",
      "Epoch: [3] [200/385]\tTime 0.379 (0.403)\tLoss 0.1829 (0.1627)\t\n",
      "Saving Model...\n",
      "Model Saved Successfully!\n",
      "Epoch: [3] [205/385]\tTime 0.421 (0.403)\tLoss 0.1535 (0.1627)\t\n",
      "Epoch: [3] [210/385]\tTime 0.390 (0.403)\tLoss 0.1685 (0.1628)\t\n",
      "Epoch: [3] [215/385]\tTime 0.446 (0.402)\tLoss 0.1647 (0.1629)\t\n",
      "Epoch: [3] [220/385]\tTime 0.370 (0.402)\tLoss 0.1725 (0.1630)\t\n",
      "Epoch: [3] [225/385]\tTime 0.427 (0.402)\tLoss 0.1620 (0.1630)\t\n",
      "Epoch: [3] [230/385]\tTime 0.401 (0.402)\tLoss 0.1677 (0.1630)\t\n",
      "Epoch: [3] [235/385]\tTime 0.448 (0.402)\tLoss 0.1563 (0.1630)\t\n",
      "Epoch: [3] [240/385]\tTime 0.378 (0.402)\tLoss 0.1784 (0.1633)\t\n",
      "Epoch: [3] [245/385]\tTime 0.434 (0.402)\tLoss 0.1669 (0.1635)\t\n",
      "Epoch: [3] [250/385]\tTime 0.378 (0.402)\tLoss 0.1536 (0.1633)\t\n",
      "Epoch: [3] [255/385]\tTime 0.442 (0.402)\tLoss 0.1721 (0.1636)\t\n",
      "Epoch: [3] [260/385]\tTime 0.433 (0.402)\tLoss 0.1695 (0.1636)\t\n",
      "Epoch: [3] [265/385]\tTime 0.443 (0.403)\tLoss 0.1575 (0.1637)\t\n",
      "Epoch: [3] [270/385]\tTime 0.463 (0.403)\tLoss 0.1482 (0.1638)\t\n",
      "Epoch: [3] [275/385]\tTime 0.384 (0.403)\tLoss 0.1675 (0.1638)\t\n",
      "Epoch: [3] [280/385]\tTime 0.388 (0.403)\tLoss 0.1658 (0.1638)\t\n",
      "Epoch: [3] [285/385]\tTime 0.362 (0.403)\tLoss 0.1747 (0.1640)\t\n",
      "Epoch: [3] [290/385]\tTime 0.376 (0.402)\tLoss 0.1764 (0.1642)\t\n",
      "Epoch: [3] [295/385]\tTime 0.443 (0.402)\tLoss 0.1898 (0.1642)\t\n",
      "Epoch: [3] [300/385]\tTime 0.428 (0.402)\tLoss 0.1762 (0.1644)\t\n",
      "Epoch: [3] [305/385]\tTime 0.440 (0.402)\tLoss 0.1920 (0.1647)\t\n",
      "Epoch: [3] [310/385]\tTime 0.431 (0.402)\tLoss 0.1781 (0.1648)\t\n",
      "Epoch: [3] [315/385]\tTime 0.386 (0.403)\tLoss 0.1587 (0.1647)\t\n",
      "Epoch: [3] [320/385]\tTime 0.394 (0.402)\tLoss 0.1758 (0.1649)\t\n",
      "Epoch: [3] [325/385]\tTime 0.402 (0.402)\tLoss 0.1732 (0.1650)\t\n",
      "Epoch: [3] [330/385]\tTime 0.431 (0.403)\tLoss 0.1853 (0.1652)\t\n",
      "Epoch: [3] [335/385]\tTime 0.438 (0.403)\tLoss 0.1642 (0.1652)\t\n",
      "Epoch: [3] [340/385]\tTime 0.429 (0.404)\tLoss 0.1561 (0.1652)\t\n",
      "Epoch: [3] [345/385]\tTime 0.395 (0.404)\tLoss 0.1818 (0.1652)\t\n",
      "Epoch: [3] [350/385]\tTime 0.362 (0.403)\tLoss 0.1547 (0.1653)\t\n",
      "Epoch: [3] [355/385]\tTime 0.433 (0.403)\tLoss 0.1629 (0.1654)\t\n",
      "Epoch: [3] [360/385]\tTime 0.401 (0.403)\tLoss 0.1453 (0.1653)\t\n",
      "Epoch: [3] [365/385]\tTime 0.384 (0.404)\tLoss 0.1656 (0.1653)\t\n",
      "Epoch: [3] [370/385]\tTime 0.386 (0.403)\tLoss 0.1716 (0.1654)\t\n",
      "Epoch: [3] [375/385]\tTime 0.485 (0.403)\tLoss 0.1811 (0.1655)\t\n",
      "Epoch: [3] [380/385]\tTime 0.393 (0.403)\tLoss 0.1838 (0.1657)\t\n",
      "Saving Model...\n",
      "Model Saved Successfully!\n",
      "Epoch: [4] [0/385]\tTime 0.412 (0.412)\tLoss 0.1219 (0.1219)\t\n",
      "Saving Model...\n",
      "Model Saved Successfully!\n",
      "Epoch: [4] [5/385]\tTime 0.417 (0.428)\tLoss 0.1172 (0.1114)\t\n",
      "Epoch: [4] [10/385]\tTime 0.397 (0.412)\tLoss 0.1043 (0.1101)\t\n",
      "Epoch: [4] [15/385]\tTime 0.365 (0.403)\tLoss 0.1143 (0.1115)\t\n",
      "Epoch: [4] [20/385]\tTime 0.386 (0.399)\tLoss 0.0948 (0.1105)\t\n",
      "Epoch: [4] [25/385]\tTime 0.371 (0.399)\tLoss 0.1142 (0.1112)\t\n",
      "Epoch: [4] [30/385]\tTime 0.374 (0.397)\tLoss 0.1058 (0.1112)\t\n",
      "Epoch: [4] [35/385]\tTime 0.428 (0.397)\tLoss 0.1120 (0.1115)\t\n",
      "Epoch: [4] [40/385]\tTime 0.400 (0.399)\tLoss 0.0954 (0.1115)\t\n",
      "Epoch: [4] [45/385]\tTime 0.397 (0.397)\tLoss 0.1147 (0.1117)\t\n",
      "Epoch: [4] [50/385]\tTime 0.384 (0.397)\tLoss 0.0986 (0.1119)\t\n",
      "Epoch: [4] [55/385]\tTime 0.344 (0.395)\tLoss 0.1015 (0.1114)\t\n",
      "Epoch: [4] [60/385]\tTime 0.403 (0.395)\tLoss 0.1043 (0.1116)\t\n",
      "Epoch: [4] [65/385]\tTime 0.419 (0.397)\tLoss 0.0982 (0.1122)\t\n",
      "Epoch: [4] [70/385]\tTime 0.415 (0.397)\tLoss 0.1287 (0.1124)\t\n",
      "Epoch: [4] [75/385]\tTime 0.428 (0.399)\tLoss 0.1193 (0.1128)\t\n",
      "Epoch: [4] [80/385]\tTime 0.422 (0.400)\tLoss 0.1188 (0.1133)\t\n",
      "Epoch: [4] [85/385]\tTime 0.363 (0.400)\tLoss 0.1143 (0.1137)\t\n",
      "Epoch: [4] [90/385]\tTime 0.376 (0.398)\tLoss 0.1187 (0.1138)\t\n",
      "Epoch: [4] [95/385]\tTime 0.393 (0.397)\tLoss 0.1304 (0.1138)\t\n",
      "Epoch: [4] [100/385]\tTime 0.432 (0.400)\tLoss 0.1188 (0.1142)\t\n",
      "Epoch: [4] [105/385]\tTime 0.389 (0.400)\tLoss 0.1197 (0.1142)\t\n",
      "Epoch: [4] [110/385]\tTime 0.380 (0.400)\tLoss 0.1171 (0.1142)\t\n",
      "Epoch: [4] [115/385]\tTime 0.391 (0.399)\tLoss 0.1175 (0.1144)\t\n",
      "Epoch: [4] [120/385]\tTime 0.392 (0.400)\tLoss 0.1242 (0.1145)\t\n",
      "Epoch: [4] [125/385]\tTime 0.394 (0.400)\tLoss 0.1194 (0.1146)\t\n",
      "Epoch: [4] [130/385]\tTime 0.453 (0.400)\tLoss 0.1252 (0.1146)\t\n",
      "Epoch: [4] [135/385]\tTime 0.377 (0.401)\tLoss 0.1291 (0.1151)\t\n",
      "Epoch: [4] [140/385]\tTime 0.450 (0.401)\tLoss 0.1223 (0.1154)\t\n",
      "Epoch: [4] [145/385]\tTime 0.408 (0.401)\tLoss 0.1191 (0.1152)\t\n",
      "Epoch: [4] [150/385]\tTime 0.364 (0.401)\tLoss 0.1319 (0.1153)\t\n",
      "Epoch: [4] [155/385]\tTime 0.409 (0.401)\tLoss 0.1128 (0.1155)\t\n",
      "Epoch: [4] [160/385]\tTime 0.378 (0.400)\tLoss 0.1262 (0.1157)\t\n",
      "Epoch: [4] [165/385]\tTime 0.367 (0.399)\tLoss 0.1112 (0.1158)\t\n",
      "Epoch: [4] [170/385]\tTime 0.421 (0.401)\tLoss 0.1271 (0.1160)\t\n",
      "Epoch: [4] [175/385]\tTime 0.396 (0.401)\tLoss 0.1461 (0.1163)\t\n",
      "Epoch: [4] [180/385]\tTime 0.398 (0.401)\tLoss 0.1194 (0.1165)\t\n",
      "Epoch: [4] [185/385]\tTime 0.377 (0.400)\tLoss 0.1281 (0.1168)\t\n",
      "Epoch: [4] [190/385]\tTime 0.398 (0.400)\tLoss 0.1066 (0.1169)\t\n",
      "Epoch: [4] [195/385]\tTime 0.413 (0.400)\tLoss 0.1040 (0.1168)\t\n",
      "Epoch: [4] [200/385]\tTime 0.371 (0.399)\tLoss 0.1248 (0.1168)\t\n",
      "Saving Model...\n",
      "Model Saved Successfully!\n",
      "Epoch: [4] [205/385]\tTime 0.403 (0.400)\tLoss 0.1204 (0.1169)\t\n",
      "Epoch: [4] [210/385]\tTime 0.529 (0.401)\tLoss 0.1119 (0.1170)\t\n",
      "Epoch: [4] [215/385]\tTime 0.379 (0.402)\tLoss 0.1181 (0.1171)\t\n",
      "Epoch: [4] [220/385]\tTime 0.371 (0.402)\tLoss 0.1265 (0.1172)\t\n",
      "Epoch: [4] [225/385]\tTime 0.376 (0.401)\tLoss 0.1350 (0.1174)\t\n",
      "Epoch: [4] [230/385]\tTime 0.445 (0.401)\tLoss 0.1205 (0.1176)\t\n",
      "Epoch: [4] [235/385]\tTime 0.422 (0.401)\tLoss 0.1140 (0.1175)\t\n",
      "Epoch: [4] [240/385]\tTime 0.468 (0.402)\tLoss 0.1473 (0.1177)\t\n",
      "Epoch: [4] [245/385]\tTime 0.413 (0.402)\tLoss 0.1298 (0.1179)\t\n",
      "Epoch: [4] [250/385]\tTime 0.370 (0.402)\tLoss 0.1238 (0.1179)\t\n",
      "Epoch: [4] [255/385]\tTime 0.364 (0.402)\tLoss 0.1146 (0.1180)\t\n",
      "Epoch: [4] [260/385]\tTime 0.372 (0.402)\tLoss 0.1231 (0.1181)\t\n",
      "Epoch: [4] [265/385]\tTime 0.399 (0.402)\tLoss 0.1407 (0.1184)\t\n",
      "Epoch: [4] [270/385]\tTime 0.404 (0.401)\tLoss 0.1068 (0.1184)\t\n",
      "Epoch: [4] [275/385]\tTime 0.416 (0.402)\tLoss 0.1326 (0.1186)\t\n",
      "Epoch: [4] [280/385]\tTime 0.393 (0.402)\tLoss 0.1325 (0.1186)\t\n",
      "Epoch: [4] [285/385]\tTime 0.393 (0.402)\tLoss 0.1254 (0.1188)\t\n",
      "Epoch: [4] [290/385]\tTime 0.397 (0.401)\tLoss 0.1062 (0.1188)\t\n",
      "Epoch: [4] [295/385]\tTime 0.408 (0.402)\tLoss 0.1245 (0.1188)\t\n",
      "Epoch: [4] [300/385]\tTime 0.418 (0.402)\tLoss 0.1219 (0.1189)\t\n",
      "Epoch: [4] [305/385]\tTime 0.423 (0.403)\tLoss 0.1230 (0.1190)\t\n",
      "Epoch: [4] [310/385]\tTime 0.438 (0.403)\tLoss 0.1253 (0.1190)\t\n",
      "Epoch: [4] [315/385]\tTime 0.389 (0.403)\tLoss 0.1296 (0.1192)\t\n",
      "Epoch: [4] [320/385]\tTime 0.509 (0.403)\tLoss 0.1229 (0.1194)\t\n",
      "Epoch: [4] [325/385]\tTime 0.408 (0.403)\tLoss 0.1552 (0.1195)\t\n",
      "Epoch: [4] [330/385]\tTime 0.402 (0.402)\tLoss 0.1341 (0.1196)\t\n",
      "Epoch: [4] [335/385]\tTime 0.378 (0.402)\tLoss 0.1239 (0.1197)\t\n",
      "Epoch: [4] [340/385]\tTime 0.356 (0.402)\tLoss 0.1197 (0.1198)\t\n",
      "Epoch: [4] [345/385]\tTime 0.360 (0.402)\tLoss 0.1353 (0.1200)\t\n",
      "Epoch: [4] [350/385]\tTime 0.382 (0.402)\tLoss 0.1126 (0.1202)\t\n",
      "Epoch: [4] [355/385]\tTime 0.432 (0.402)\tLoss 0.1253 (0.1203)\t\n",
      "Epoch: [4] [360/385]\tTime 0.437 (0.403)\tLoss 0.1356 (0.1205)\t\n",
      "Epoch: [4] [365/385]\tTime 0.420 (0.403)\tLoss 0.1338 (0.1206)\t\n",
      "Epoch: [4] [370/385]\tTime 0.399 (0.403)\tLoss 0.1207 (0.1206)\t\n",
      "Epoch: [4] [375/385]\tTime 0.443 (0.403)\tLoss 0.1176 (0.1206)\t\n",
      "Epoch: [4] [380/385]\tTime 0.382 (0.403)\tLoss 0.1340 (0.1207)\t\n",
      "Saving Model...\n",
      "Model Saved Successfully!\n",
      "Epoch: [5] [0/385]\tTime 0.482 (0.482)\tLoss 0.0835 (0.0835)\t\n",
      "Saving Model...\n",
      "Model Saved Successfully!\n",
      "Epoch: [5] [5/385]\tTime 0.384 (0.425)\tLoss 0.0898 (0.0832)\t\n",
      "Epoch: [5] [10/385]\tTime 0.374 (0.404)\tLoss 0.0818 (0.0853)\t\n",
      "Epoch: [5] [15/385]\tTime 0.363 (0.403)\tLoss 0.0881 (0.0846)\t\n",
      "Epoch: [5] [20/385]\tTime 0.372 (0.405)\tLoss 0.0904 (0.0836)\t\n",
      "Epoch: [5] [25/385]\tTime 0.360 (0.400)\tLoss 0.0802 (0.0827)\t\n",
      "Epoch: [5] [30/385]\tTime 0.369 (0.398)\tLoss 0.0803 (0.0823)\t\n",
      "Epoch: [5] [35/385]\tTime 0.405 (0.398)\tLoss 0.0727 (0.0827)\t\n",
      "Epoch: [5] [40/385]\tTime 0.421 (0.397)\tLoss 0.0961 (0.0826)\t\n",
      "Epoch: [5] [45/385]\tTime 0.420 (0.400)\tLoss 0.0804 (0.0827)\t\n",
      "Epoch: [5] [50/385]\tTime 0.405 (0.401)\tLoss 0.0857 (0.0823)\t\n",
      "Epoch: [5] [55/385]\tTime 0.413 (0.399)\tLoss 0.0908 (0.0822)\t\n",
      "Epoch: [5] [60/385]\tTime 0.461 (0.401)\tLoss 0.0886 (0.0822)\t\n",
      "Epoch: [5] [65/385]\tTime 0.412 (0.401)\tLoss 0.0871 (0.0823)\t\n",
      "Epoch: [5] [70/385]\tTime 0.388 (0.400)\tLoss 0.0740 (0.0824)\t\n",
      "Epoch: [5] [75/385]\tTime 0.406 (0.402)\tLoss 0.0811 (0.0822)\t\n",
      "Epoch: [5] [80/385]\tTime 0.435 (0.402)\tLoss 0.0799 (0.0820)\t\n",
      "Epoch: [5] [85/385]\tTime 0.360 (0.402)\tLoss 0.0772 (0.0823)\t\n",
      "Epoch: [5] [90/385]\tTime 0.430 (0.402)\tLoss 0.0808 (0.0825)\t\n",
      "Epoch: [5] [95/385]\tTime 0.392 (0.403)\tLoss 0.0704 (0.0823)\t\n",
      "Epoch: [5] [100/385]\tTime 0.433 (0.403)\tLoss 0.0838 (0.0822)\t\n",
      "Epoch: [5] [105/385]\tTime 0.436 (0.403)\tLoss 0.0807 (0.0824)\t\n",
      "Epoch: [5] [110/385]\tTime 0.389 (0.402)\tLoss 0.0874 (0.0824)\t\n",
      "Epoch: [5] [115/385]\tTime 0.385 (0.402)\tLoss 0.0790 (0.0825)\t\n",
      "Epoch: [5] [120/385]\tTime 0.391 (0.402)\tLoss 0.0781 (0.0825)\t\n",
      "Epoch: [5] [125/385]\tTime 0.459 (0.404)\tLoss 0.0837 (0.0828)\t\n",
      "Epoch: [5] [130/385]\tTime 0.365 (0.404)\tLoss 0.0760 (0.0828)\t\n",
      "Epoch: [5] [135/385]\tTime 0.381 (0.403)\tLoss 0.0892 (0.0829)\t\n",
      "Epoch: [5] [140/385]\tTime 0.434 (0.403)\tLoss 0.0921 (0.0831)\t\n",
      "Epoch: [5] [145/385]\tTime 0.400 (0.404)\tLoss 0.0836 (0.0832)\t\n",
      "Epoch: [5] [150/385]\tTime 0.372 (0.404)\tLoss 0.0832 (0.0833)\t\n",
      "Epoch: [5] [155/385]\tTime 0.432 (0.403)\tLoss 0.0860 (0.0833)\t\n",
      "Epoch: [5] [160/385]\tTime 0.389 (0.403)\tLoss 0.0847 (0.0834)\t\n",
      "Epoch: [5] [165/385]\tTime 0.403 (0.404)\tLoss 0.0851 (0.0836)\t\n",
      "Epoch: [5] [170/385]\tTime 0.410 (0.404)\tLoss 0.0814 (0.0838)\t\n",
      "Epoch: [5] [175/385]\tTime 0.396 (0.403)\tLoss 0.1027 (0.0838)\t\n",
      "Epoch: [5] [180/385]\tTime 0.463 (0.404)\tLoss 0.0984 (0.0841)\t\n",
      "Epoch: [5] [185/385]\tTime 0.408 (0.404)\tLoss 0.0844 (0.0842)\t\n",
      "Epoch: [5] [190/385]\tTime 0.389 (0.405)\tLoss 0.0931 (0.0844)\t\n",
      "Epoch: [5] [195/385]\tTime 0.423 (0.404)\tLoss 0.0926 (0.0843)\t\n",
      "Epoch: [5] [200/385]\tTime 0.380 (0.404)\tLoss 0.0857 (0.0845)\t\n",
      "Saving Model...\n",
      "Model Saved Successfully!\n",
      "Epoch: [5] [205/385]\tTime 0.391 (0.404)\tLoss 0.0888 (0.0846)\t\n",
      "Epoch: [5] [210/385]\tTime 0.408 (0.404)\tLoss 0.0998 (0.0848)\t\n",
      "Epoch: [5] [215/385]\tTime 0.354 (0.404)\tLoss 0.0874 (0.0848)\t\n",
      "Epoch: [5] [220/385]\tTime 0.433 (0.403)\tLoss 0.0907 (0.0849)\t\n",
      "Epoch: [5] [225/385]\tTime 0.372 (0.403)\tLoss 0.0867 (0.0851)\t\n",
      "Epoch: [5] [230/385]\tTime 0.421 (0.403)\tLoss 0.0802 (0.0850)\t\n",
      "Epoch: [5] [235/385]\tTime 0.387 (0.403)\tLoss 0.1035 (0.0852)\t\n",
      "Epoch: [5] [240/385]\tTime 0.395 (0.402)\tLoss 0.0907 (0.0853)\t\n",
      "Epoch: [5] [245/385]\tTime 0.410 (0.403)\tLoss 0.0898 (0.0855)\t\n",
      "Epoch: [5] [250/385]\tTime 0.528 (0.404)\tLoss 0.1033 (0.0856)\t\n",
      "Epoch: [5] [255/385]\tTime 0.395 (0.404)\tLoss 0.1023 (0.0860)\t\n",
      "Epoch: [5] [260/385]\tTime 0.417 (0.404)\tLoss 0.0962 (0.0860)\t\n",
      "Epoch: [5] [265/385]\tTime 0.404 (0.404)\tLoss 0.0985 (0.0862)\t\n",
      "Epoch: [5] [270/385]\tTime 0.397 (0.404)\tLoss 0.0846 (0.0862)\t\n",
      "Epoch: [5] [275/385]\tTime 0.359 (0.403)\tLoss 0.1014 (0.0864)\t\n",
      "Epoch: [5] [280/385]\tTime 0.399 (0.403)\tLoss 0.0968 (0.0865)\t\n",
      "Epoch: [5] [285/385]\tTime 0.410 (0.404)\tLoss 0.0961 (0.0866)\t\n",
      "Epoch: [5] [290/385]\tTime 0.485 (0.404)\tLoss 0.1098 (0.0867)\t\n",
      "Epoch: [5] [295/385]\tTime 0.534 (0.404)\tLoss 0.0936 (0.0869)\t\n",
      "Epoch: [5] [300/385]\tTime 0.411 (0.404)\tLoss 0.0871 (0.0870)\t\n",
      "Epoch: [5] [305/385]\tTime 0.406 (0.405)\tLoss 0.0823 (0.0870)\t\n",
      "Epoch: [5] [310/385]\tTime 0.423 (0.404)\tLoss 0.0979 (0.0872)\t\n",
      "Epoch: [5] [315/385]\tTime 0.404 (0.404)\tLoss 0.1066 (0.0873)\t\n",
      "Epoch: [5] [320/385]\tTime 0.406 (0.404)\tLoss 0.0871 (0.0874)\t\n",
      "Epoch: [5] [325/385]\tTime 0.420 (0.404)\tLoss 0.1014 (0.0875)\t\n",
      "Epoch: [5] [330/385]\tTime 0.371 (0.404)\tLoss 0.0882 (0.0876)\t\n",
      "Epoch: [5] [335/385]\tTime 0.381 (0.404)\tLoss 0.0985 (0.0877)\t\n",
      "Epoch: [5] [340/385]\tTime 0.363 (0.404)\tLoss 0.1068 (0.0878)\t\n",
      "Epoch: [5] [345/385]\tTime 0.480 (0.404)\tLoss 0.0927 (0.0880)\t\n",
      "Epoch: [5] [350/385]\tTime 0.412 (0.404)\tLoss 0.1041 (0.0881)\t\n",
      "Epoch: [5] [355/385]\tTime 0.460 (0.404)\tLoss 0.0944 (0.0883)\t\n",
      "Epoch: [5] [360/385]\tTime 0.389 (0.404)\tLoss 0.0912 (0.0884)\t\n",
      "Epoch: [5] [365/385]\tTime 0.399 (0.404)\tLoss 0.0778 (0.0885)\t\n",
      "Epoch: [5] [370/385]\tTime 0.419 (0.404)\tLoss 0.0885 (0.0886)\t\n",
      "Epoch: [5] [375/385]\tTime 0.406 (0.404)\tLoss 0.1019 (0.0887)\t\n",
      "Epoch: [5] [380/385]\tTime 0.374 (0.404)\tLoss 0.1076 (0.0889)\t\n",
      "Saving Model...\n",
      "Model Saved Successfully!\n",
      "Epoch: [6] [0/385]\tTime 0.445 (0.445)\tLoss 0.0598 (0.0598)\t\n",
      "Saving Model...\n",
      "Model Saved Successfully!\n",
      "Epoch: [6] [5/385]\tTime 0.480 (0.449)\tLoss 0.0635 (0.0621)\t\n",
      "Epoch: [6] [10/385]\tTime 0.412 (0.442)\tLoss 0.0636 (0.0613)\t\n",
      "Epoch: [6] [15/385]\tTime 0.432 (0.432)\tLoss 0.0560 (0.0607)\t\n",
      "Epoch: [6] [20/385]\tTime 0.439 (0.421)\tLoss 0.0619 (0.0607)\t\n",
      "Epoch: [6] [25/385]\tTime 0.393 (0.418)\tLoss 0.0582 (0.0605)\t\n",
      "Epoch: [6] [30/385]\tTime 0.363 (0.412)\tLoss 0.0539 (0.0603)\t\n",
      "Epoch: [6] [35/385]\tTime 0.366 (0.408)\tLoss 0.0650 (0.0604)\t\n",
      "Epoch: [6] [40/385]\tTime 0.443 (0.407)\tLoss 0.0633 (0.0606)\t\n",
      "Epoch: [6] [45/385]\tTime 0.411 (0.407)\tLoss 0.0678 (0.0605)\t\n",
      "Epoch: [6] [50/385]\tTime 0.373 (0.405)\tLoss 0.0576 (0.0605)\t\n",
      "Epoch: [6] [55/385]\tTime 0.417 (0.404)\tLoss 0.0637 (0.0605)\t\n",
      "Epoch: [6] [60/385]\tTime 0.370 (0.401)\tLoss 0.0558 (0.0605)\t\n",
      "Epoch: [6] [65/385]\tTime 0.462 (0.402)\tLoss 0.0617 (0.0606)\t\n",
      "Epoch: [6] [70/385]\tTime 0.426 (0.404)\tLoss 0.0547 (0.0606)\t\n",
      "Epoch: [6] [75/385]\tTime 0.440 (0.407)\tLoss 0.0545 (0.0606)\t\n",
      "Epoch: [6] [80/385]\tTime 0.431 (0.408)\tLoss 0.0586 (0.0607)\t\n",
      "Epoch: [6] [85/385]\tTime 0.411 (0.409)\tLoss 0.0666 (0.0611)\t\n",
      "Epoch: [6] [90/385]\tTime 0.394 (0.408)\tLoss 0.0606 (0.0610)\t\n",
      "Epoch: [6] [95/385]\tTime 0.406 (0.408)\tLoss 0.0684 (0.0611)\t\n",
      "Epoch: [6] [100/385]\tTime 0.423 (0.409)\tLoss 0.0675 (0.0612)\t\n",
      "Epoch: [6] [105/385]\tTime 0.428 (0.409)\tLoss 0.0505 (0.0613)\t\n",
      "Epoch: [6] [110/385]\tTime 0.401 (0.409)\tLoss 0.0664 (0.0613)\t\n",
      "Epoch: [6] [115/385]\tTime 0.357 (0.408)\tLoss 0.0681 (0.0614)\t\n",
      "Epoch: [6] [120/385]\tTime 0.407 (0.407)\tLoss 0.0541 (0.0614)\t\n",
      "Epoch: [6] [125/385]\tTime 0.384 (0.408)\tLoss 0.0621 (0.0616)\t\n",
      "Epoch: [6] [130/385]\tTime 0.364 (0.408)\tLoss 0.0650 (0.0616)\t\n",
      "Epoch: [6] [135/385]\tTime 0.404 (0.408)\tLoss 0.0644 (0.0616)\t\n",
      "Epoch: [6] [140/385]\tTime 0.451 (0.408)\tLoss 0.0585 (0.0617)\t\n",
      "Epoch: [6] [145/385]\tTime 0.436 (0.408)\tLoss 0.0684 (0.0618)\t\n",
      "Epoch: [6] [150/385]\tTime 0.368 (0.407)\tLoss 0.0674 (0.0618)\t\n",
      "Epoch: [6] [155/385]\tTime 0.358 (0.406)\tLoss 0.0651 (0.0618)\t\n",
      "Epoch: [6] [160/385]\tTime 0.500 (0.407)\tLoss 0.0646 (0.0619)\t\n",
      "Epoch: [6] [165/385]\tTime 0.406 (0.408)\tLoss 0.0584 (0.0619)\t\n",
      "Epoch: [6] [170/385]\tTime 0.384 (0.408)\tLoss 0.0663 (0.0620)\t\n",
      "Epoch: [6] [175/385]\tTime 0.403 (0.408)\tLoss 0.0624 (0.0621)\t\n",
      "Epoch: [6] [180/385]\tTime 0.371 (0.407)\tLoss 0.0654 (0.0622)\t\n",
      "Epoch: [6] [185/385]\tTime 0.415 (0.407)\tLoss 0.0640 (0.0623)\t\n",
      "Epoch: [6] [190/385]\tTime 0.361 (0.406)\tLoss 0.0714 (0.0624)\t\n",
      "Epoch: [6] [195/385]\tTime 0.395 (0.406)\tLoss 0.0670 (0.0625)\t\n",
      "Epoch: [6] [200/385]\tTime 0.414 (0.406)\tLoss 0.0674 (0.0626)\t\n",
      "Saving Model...\n",
      "Model Saved Successfully!\n",
      "Epoch: [6] [205/385]\tTime 0.442 (0.406)\tLoss 0.0683 (0.0627)\t\n",
      "Epoch: [6] [210/385]\tTime 0.390 (0.406)\tLoss 0.0618 (0.0627)\t\n",
      "Epoch: [6] [215/385]\tTime 0.367 (0.406)\tLoss 0.0608 (0.0627)\t\n",
      "Epoch: [6] [220/385]\tTime 0.371 (0.405)\tLoss 0.0723 (0.0628)\t\n",
      "Epoch: [6] [225/385]\tTime 0.418 (0.405)\tLoss 0.0708 (0.0629)\t\n",
      "Epoch: [6] [230/385]\tTime 0.376 (0.405)\tLoss 0.0745 (0.0630)\t\n",
      "Epoch: [6] [235/385]\tTime 0.367 (0.405)\tLoss 0.0604 (0.0631)\t\n",
      "Epoch: [6] [240/385]\tTime 0.413 (0.405)\tLoss 0.0735 (0.0633)\t\n",
      "Epoch: [6] [245/385]\tTime 0.412 (0.405)\tLoss 0.0687 (0.0634)\t\n",
      "Epoch: [6] [250/385]\tTime 0.408 (0.406)\tLoss 0.0698 (0.0636)\t\n",
      "Epoch: [6] [255/385]\tTime 0.422 (0.406)\tLoss 0.0725 (0.0638)\t\n",
      "Epoch: [6] [260/385]\tTime 0.392 (0.406)\tLoss 0.0722 (0.0639)\t\n",
      "Epoch: [6] [265/385]\tTime 0.346 (0.405)\tLoss 0.0668 (0.0639)\t\n",
      "Epoch: [6] [270/385]\tTime 0.451 (0.405)\tLoss 0.0718 (0.0641)\t\n",
      "Epoch: [6] [275/385]\tTime 0.376 (0.405)\tLoss 0.0771 (0.0641)\t\n",
      "Epoch: [6] [280/385]\tTime 0.406 (0.405)\tLoss 0.0791 (0.0643)\t\n",
      "Epoch: [6] [285/385]\tTime 0.415 (0.406)\tLoss 0.0741 (0.0644)\t\n",
      "Epoch: [6] [290/385]\tTime 0.364 (0.406)\tLoss 0.0707 (0.0645)\t\n",
      "Epoch: [6] [295/385]\tTime 0.351 (0.406)\tLoss 0.0694 (0.0645)\t\n",
      "Epoch: [6] [300/385]\tTime 0.412 (0.405)\tLoss 0.0700 (0.0647)\t\n",
      "Epoch: [6] [305/385]\tTime 0.380 (0.405)\tLoss 0.0836 (0.0648)\t\n",
      "Epoch: [6] [310/385]\tTime 0.480 (0.405)\tLoss 0.0722 (0.0649)\t\n",
      "Epoch: [6] [315/385]\tTime 0.398 (0.405)\tLoss 0.0656 (0.0649)\t\n",
      "Epoch: [6] [320/385]\tTime 0.452 (0.406)\tLoss 0.0710 (0.0652)\t\n",
      "Epoch: [6] [325/385]\tTime 0.372 (0.406)\tLoss 0.0651 (0.0652)\t\n",
      "Epoch: [6] [330/385]\tTime 0.372 (0.405)\tLoss 0.0711 (0.0653)\t\n",
      "Epoch: [6] [335/385]\tTime 0.393 (0.405)\tLoss 0.0734 (0.0654)\t\n",
      "Epoch: [6] [340/385]\tTime 0.391 (0.405)\tLoss 0.0803 (0.0656)\t\n",
      "Epoch: [6] [345/385]\tTime 0.433 (0.406)\tLoss 0.0679 (0.0657)\t\n",
      "Epoch: [6] [350/385]\tTime 0.379 (0.406)\tLoss 0.0691 (0.0657)\t\n",
      "Epoch: [6] [355/385]\tTime 0.381 (0.406)\tLoss 0.0668 (0.0658)\t\n",
      "Epoch: [6] [360/385]\tTime 0.406 (0.405)\tLoss 0.0712 (0.0658)\t\n",
      "Epoch: [6] [365/385]\tTime 0.436 (0.405)\tLoss 0.0740 (0.0659)\t\n",
      "Epoch: [6] [370/385]\tTime 0.410 (0.406)\tLoss 0.0846 (0.0661)\t\n",
      "Epoch: [6] [375/385]\tTime 0.422 (0.406)\tLoss 0.0784 (0.0662)\t\n",
      "Epoch: [6] [380/385]\tTime 0.380 (0.406)\tLoss 0.0878 (0.0664)\t\n",
      "Saving Model...\n",
      "Model Saved Successfully!\n",
      "Epoch: [7] [0/385]\tTime 0.437 (0.437)\tLoss 0.0459 (0.0459)\t\n",
      "Saving Model...\n",
      "Model Saved Successfully!\n",
      "Epoch: [7] [5/385]\tTime 0.431 (0.470)\tLoss 0.0474 (0.0485)\t\n",
      "Epoch: [7] [10/385]\tTime 0.396 (0.436)\tLoss 0.0388 (0.0477)\t\n",
      "Epoch: [7] [15/385]\tTime 0.402 (0.420)\tLoss 0.0429 (0.0462)\t\n",
      "Epoch: [7] [20/385]\tTime 0.465 (0.419)\tLoss 0.0487 (0.0472)\t\n",
      "Epoch: [7] [25/385]\tTime 0.403 (0.416)\tLoss 0.0510 (0.0466)\t\n",
      "Epoch: [7] [30/385]\tTime 0.388 (0.412)\tLoss 0.0432 (0.0465)\t\n",
      "Epoch: [7] [35/385]\tTime 0.404 (0.411)\tLoss 0.0480 (0.0460)\t\n",
      "Epoch: [7] [40/385]\tTime 0.431 (0.412)\tLoss 0.0465 (0.0460)\t\n",
      "Epoch: [7] [45/385]\tTime 0.404 (0.411)\tLoss 0.0468 (0.0459)\t\n",
      "Epoch: [7] [50/385]\tTime 0.379 (0.411)\tLoss 0.0483 (0.0459)\t\n",
      "Epoch: [7] [55/385]\tTime 0.381 (0.408)\tLoss 0.0479 (0.0458)\t\n",
      "Epoch: [7] [60/385]\tTime 0.362 (0.407)\tLoss 0.0481 (0.0456)\t\n",
      "Epoch: [7] [65/385]\tTime 0.470 (0.407)\tLoss 0.0498 (0.0459)\t\n",
      "Epoch: [7] [70/385]\tTime 0.367 (0.405)\tLoss 0.0421 (0.0457)\t\n",
      "Epoch: [7] [75/385]\tTime 0.444 (0.406)\tLoss 0.0498 (0.0457)\t\n",
      "Epoch: [7] [80/385]\tTime 0.410 (0.406)\tLoss 0.0448 (0.0458)\t\n",
      "Epoch: [7] [85/385]\tTime 0.360 (0.405)\tLoss 0.0441 (0.0458)\t\n",
      "Epoch: [7] [90/385]\tTime 0.395 (0.405)\tLoss 0.0382 (0.0459)\t\n",
      "Epoch: [7] [95/385]\tTime 0.392 (0.405)\tLoss 0.0433 (0.0460)\t\n",
      "Epoch: [7] [100/385]\tTime 0.385 (0.406)\tLoss 0.0431 (0.0460)\t\n",
      "Epoch: [7] [105/385]\tTime 0.411 (0.405)\tLoss 0.0533 (0.0461)\t\n",
      "Epoch: [7] [110/385]\tTime 0.406 (0.404)\tLoss 0.0469 (0.0462)\t\n",
      "Epoch: [7] [115/385]\tTime 0.399 (0.405)\tLoss 0.0471 (0.0462)\t\n",
      "Epoch: [7] [120/385]\tTime 0.419 (0.406)\tLoss 0.0472 (0.0463)\t\n",
      "Epoch: [7] [125/385]\tTime 0.400 (0.405)\tLoss 0.0495 (0.0464)\t\n",
      "Epoch: [7] [130/385]\tTime 0.447 (0.406)\tLoss 0.0541 (0.0466)\t\n",
      "Epoch: [7] [135/385]\tTime 0.392 (0.405)\tLoss 0.0447 (0.0467)\t\n",
      "Epoch: [7] [140/385]\tTime 0.381 (0.405)\tLoss 0.0514 (0.0469)\t\n",
      "Epoch: [7] [145/385]\tTime 0.514 (0.405)\tLoss 0.0509 (0.0470)\t\n",
      "Epoch: [7] [150/385]\tTime 0.394 (0.405)\tLoss 0.0473 (0.0470)\t\n",
      "Epoch: [7] [155/385]\tTime 0.356 (0.405)\tLoss 0.0451 (0.0470)\t\n",
      "Epoch: [7] [160/385]\tTime 0.435 (0.405)\tLoss 0.0469 (0.0470)\t\n",
      "Epoch: [7] [165/385]\tTime 0.415 (0.404)\tLoss 0.0546 (0.0472)\t\n",
      "Epoch: [7] [170/385]\tTime 0.387 (0.404)\tLoss 0.0499 (0.0473)\t\n",
      "Epoch: [7] [175/385]\tTime 0.400 (0.404)\tLoss 0.0483 (0.0474)\t\n",
      "Epoch: [7] [180/385]\tTime 0.371 (0.403)\tLoss 0.0620 (0.0476)\t\n",
      "Epoch: [7] [185/385]\tTime 0.407 (0.403)\tLoss 0.0566 (0.0477)\t\n",
      "Epoch: [7] [190/385]\tTime 0.403 (0.403)\tLoss 0.0488 (0.0478)\t\n",
      "Epoch: [7] [195/385]\tTime 0.402 (0.403)\tLoss 0.0457 (0.0479)\t\n",
      "Epoch: [7] [200/385]\tTime 0.378 (0.403)\tLoss 0.0434 (0.0479)\t\n",
      "Saving Model...\n",
      "Model Saved Successfully!\n",
      "Epoch: [7] [205/385]\tTime 0.405 (0.404)\tLoss 0.0502 (0.0480)\t\n",
      "Epoch: [7] [210/385]\tTime 0.364 (0.404)\tLoss 0.0472 (0.0481)\t\n",
      "Epoch: [7] [215/385]\tTime 0.391 (0.403)\tLoss 0.0499 (0.0482)\t\n",
      "Epoch: [7] [220/385]\tTime 0.377 (0.403)\tLoss 0.0553 (0.0483)\t\n",
      "Epoch: [7] [225/385]\tTime 0.374 (0.403)\tLoss 0.0509 (0.0483)\t\n",
      "Epoch: [7] [230/385]\tTime 0.365 (0.403)\tLoss 0.0469 (0.0483)\t\n",
      "Epoch: [7] [235/385]\tTime 0.497 (0.403)\tLoss 0.0543 (0.0484)\t\n",
      "Epoch: [7] [240/385]\tTime 0.426 (0.403)\tLoss 0.0519 (0.0486)\t\n",
      "Epoch: [7] [245/385]\tTime 0.408 (0.404)\tLoss 0.0531 (0.0486)\t\n",
      "Epoch: [7] [250/385]\tTime 0.431 (0.404)\tLoss 0.0561 (0.0487)\t\n",
      "Epoch: [7] [255/385]\tTime 0.385 (0.403)\tLoss 0.0470 (0.0488)\t\n",
      "Epoch: [7] [260/385]\tTime 0.433 (0.403)\tLoss 0.0535 (0.0489)\t\n",
      "Epoch: [7] [265/385]\tTime 0.443 (0.404)\tLoss 0.0534 (0.0489)\t\n",
      "Epoch: [7] [270/385]\tTime 0.384 (0.403)\tLoss 0.0542 (0.0490)\t\n",
      "Epoch: [7] [275/385]\tTime 0.417 (0.403)\tLoss 0.0598 (0.0492)\t\n",
      "Epoch: [7] [280/385]\tTime 0.361 (0.403)\tLoss 0.0621 (0.0493)\t\n",
      "Epoch: [7] [285/385]\tTime 0.361 (0.403)\tLoss 0.0507 (0.0494)\t\n",
      "Epoch: [7] [290/385]\tTime 0.394 (0.403)\tLoss 0.0492 (0.0495)\t\n",
      "Epoch: [7] [295/385]\tTime 0.429 (0.403)\tLoss 0.0568 (0.0497)\t\n",
      "Epoch: [7] [300/385]\tTime 0.462 (0.403)\tLoss 0.0472 (0.0497)\t\n",
      "Epoch: [7] [305/385]\tTime 0.536 (0.404)\tLoss 0.0541 (0.0498)\t\n",
      "Epoch: [7] [310/385]\tTime 0.393 (0.404)\tLoss 0.0564 (0.0498)\t\n",
      "Epoch: [7] [315/385]\tTime 0.364 (0.404)\tLoss 0.0542 (0.0500)\t\n",
      "Epoch: [7] [320/385]\tTime 0.370 (0.404)\tLoss 0.0563 (0.0501)\t\n",
      "Epoch: [7] [325/385]\tTime 0.414 (0.404)\tLoss 0.0614 (0.0502)\t\n",
      "Epoch: [7] [330/385]\tTime 0.424 (0.404)\tLoss 0.0526 (0.0503)\t\n",
      "Epoch: [7] [335/385]\tTime 0.389 (0.404)\tLoss 0.0617 (0.0504)\t\n",
      "Epoch: [7] [340/385]\tTime 0.468 (0.404)\tLoss 0.0552 (0.0505)\t\n",
      "Epoch: [7] [345/385]\tTime 0.361 (0.404)\tLoss 0.0605 (0.0506)\t\n",
      "Epoch: [7] [350/385]\tTime 0.418 (0.404)\tLoss 0.0526 (0.0507)\t\n",
      "Epoch: [7] [355/385]\tTime 0.376 (0.404)\tLoss 0.0560 (0.0507)\t\n",
      "Epoch: [7] [360/385]\tTime 0.388 (0.404)\tLoss 0.0602 (0.0508)\t\n",
      "Epoch: [7] [365/385]\tTime 0.386 (0.404)\tLoss 0.0563 (0.0510)\t\n",
      "Epoch: [7] [370/385]\tTime 0.386 (0.404)\tLoss 0.0565 (0.0511)\t\n",
      "Epoch: [7] [375/385]\tTime 0.392 (0.405)\tLoss 0.0519 (0.0512)\t\n",
      "Epoch: [7] [380/385]\tTime 0.515 (0.405)\tLoss 0.0582 (0.0513)\t\n",
      "Saving Model...\n",
      "Model Saved Successfully!\n",
      "Epoch: [8] [0/385]\tTime 0.483 (0.483)\tLoss 0.0350 (0.0350)\t\n",
      "Saving Model...\n",
      "Model Saved Successfully!\n",
      "Epoch: [8] [5/385]\tTime 0.388 (0.419)\tLoss 0.0356 (0.0364)\t\n",
      "Epoch: [8] [10/385]\tTime 0.452 (0.419)\tLoss 0.0353 (0.0361)\t\n",
      "Epoch: [8] [15/385]\tTime 0.369 (0.407)\tLoss 0.0354 (0.0355)\t\n",
      "Epoch: [8] [20/385]\tTime 0.379 (0.408)\tLoss 0.0344 (0.0359)\t\n",
      "Epoch: [8] [25/385]\tTime 0.380 (0.403)\tLoss 0.0323 (0.0362)\t\n",
      "Epoch: [8] [30/385]\tTime 0.400 (0.401)\tLoss 0.0367 (0.0366)\t\n",
      "Epoch: [8] [35/385]\tTime 0.407 (0.401)\tLoss 0.0377 (0.0370)\t\n",
      "Epoch: [8] [40/385]\tTime 0.407 (0.403)\tLoss 0.0343 (0.0371)\t\n",
      "Epoch: [8] [45/385]\tTime 0.416 (0.403)\tLoss 0.0350 (0.0374)\t\n",
      "Epoch: [8] [50/385]\tTime 0.393 (0.401)\tLoss 0.0351 (0.0372)\t\n",
      "Epoch: [8] [55/385]\tTime 0.458 (0.405)\tLoss 0.0341 (0.0370)\t\n",
      "Epoch: [8] [60/385]\tTime 0.426 (0.405)\tLoss 0.0407 (0.0369)\t\n",
      "Epoch: [8] [65/385]\tTime 0.363 (0.403)\tLoss 0.0322 (0.0367)\t\n",
      "Epoch: [8] [70/385]\tTime 0.402 (0.403)\tLoss 0.0316 (0.0366)\t\n",
      "Epoch: [8] [75/385]\tTime 0.378 (0.402)\tLoss 0.0350 (0.0365)\t\n",
      "Epoch: [8] [80/385]\tTime 0.378 (0.401)\tLoss 0.0339 (0.0364)\t\n",
      "Epoch: [8] [85/385]\tTime 0.431 (0.401)\tLoss 0.0377 (0.0364)\t\n",
      "Epoch: [8] [90/385]\tTime 0.363 (0.400)\tLoss 0.0413 (0.0365)\t\n",
      "Epoch: [8] [95/385]\tTime 0.385 (0.400)\tLoss 0.0382 (0.0363)\t\n",
      "Epoch: [8] [100/385]\tTime 0.418 (0.399)\tLoss 0.0368 (0.0362)\t\n",
      "Epoch: [8] [105/385]\tTime 0.406 (0.400)\tLoss 0.0382 (0.0362)\t\n",
      "Epoch: [8] [110/385]\tTime 0.392 (0.399)\tLoss 0.0455 (0.0363)\t\n",
      "Epoch: [8] [115/385]\tTime 0.403 (0.399)\tLoss 0.0327 (0.0364)\t\n",
      "Epoch: [8] [120/385]\tTime 0.362 (0.399)\tLoss 0.0374 (0.0365)\t\n",
      "Epoch: [8] [125/385]\tTime 0.376 (0.398)\tLoss 0.0352 (0.0365)\t\n",
      "Epoch: [8] [130/385]\tTime 0.409 (0.399)\tLoss 0.0446 (0.0366)\t\n",
      "Epoch: [8] [135/385]\tTime 0.418 (0.399)\tLoss 0.0393 (0.0367)\t\n",
      "Epoch: [8] [140/385]\tTime 0.461 (0.400)\tLoss 0.0339 (0.0367)\t\n",
      "Epoch: [8] [145/385]\tTime 0.435 (0.401)\tLoss 0.0449 (0.0368)\t\n",
      "Epoch: [8] [150/385]\tTime 0.405 (0.401)\tLoss 0.0353 (0.0367)\t\n",
      "Epoch: [8] [155/385]\tTime 0.446 (0.402)\tLoss 0.0333 (0.0368)\t\n",
      "Epoch: [8] [160/385]\tTime 0.377 (0.403)\tLoss 0.0389 (0.0368)\t\n",
      "Epoch: [8] [165/385]\tTime 0.372 (0.403)\tLoss 0.0448 (0.0369)\t\n",
      "Epoch: [8] [170/385]\tTime 0.384 (0.403)\tLoss 0.0407 (0.0370)\t\n",
      "Epoch: [8] [175/385]\tTime 0.370 (0.403)\tLoss 0.0375 (0.0370)\t\n",
      "Epoch: [8] [180/385]\tTime 0.401 (0.402)\tLoss 0.0371 (0.0372)\t\n",
      "Epoch: [8] [185/385]\tTime 0.362 (0.401)\tLoss 0.0408 (0.0372)\t\n",
      "Epoch: [8] [190/385]\tTime 0.460 (0.402)\tLoss 0.0443 (0.0374)\t\n",
      "Epoch: [8] [195/385]\tTime 0.434 (0.404)\tLoss 0.0415 (0.0374)\t\n",
      "Epoch: [8] [200/385]\tTime 0.420 (0.405)\tLoss 0.0407 (0.0375)\t\n",
      "Saving Model...\n",
      "Model Saved Successfully!\n",
      "Epoch: [8] [205/385]\tTime 0.408 (0.405)\tLoss 0.0411 (0.0376)\t\n",
      "Epoch: [8] [210/385]\tTime 0.483 (0.405)\tLoss 0.0397 (0.0376)\t\n",
      "Epoch: [8] [215/385]\tTime 0.421 (0.405)\tLoss 0.0375 (0.0376)\t\n",
      "Epoch: [8] [220/385]\tTime 0.396 (0.405)\tLoss 0.0399 (0.0377)\t\n",
      "Epoch: [8] [225/385]\tTime 0.389 (0.405)\tLoss 0.0446 (0.0378)\t\n",
      "Epoch: [8] [230/385]\tTime 0.377 (0.405)\tLoss 0.0416 (0.0379)\t\n",
      "Epoch: [8] [235/385]\tTime 0.449 (0.405)\tLoss 0.0431 (0.0380)\t\n",
      "Epoch: [8] [240/385]\tTime 0.363 (0.404)\tLoss 0.0471 (0.0382)\t\n",
      "Epoch: [8] [245/385]\tTime 0.414 (0.404)\tLoss 0.0377 (0.0383)\t\n",
      "Epoch: [8] [250/385]\tTime 0.430 (0.404)\tLoss 0.0389 (0.0384)\t\n",
      "Epoch: [8] [255/385]\tTime 0.392 (0.404)\tLoss 0.0465 (0.0385)\t\n",
      "Epoch: [8] [260/385]\tTime 0.432 (0.404)\tLoss 0.0441 (0.0387)\t\n",
      "Epoch: [8] [265/385]\tTime 0.367 (0.403)\tLoss 0.0464 (0.0387)\t\n",
      "Epoch: [8] [270/385]\tTime 0.476 (0.403)\tLoss 0.0506 (0.0388)\t\n",
      "Epoch: [8] [275/385]\tTime 0.434 (0.403)\tLoss 0.0513 (0.0390)\t\n",
      "Epoch: [8] [280/385]\tTime 0.422 (0.404)\tLoss 0.0466 (0.0391)\t\n",
      "Epoch: [8] [285/385]\tTime 0.378 (0.404)\tLoss 0.0394 (0.0393)\t\n",
      "Epoch: [8] [290/385]\tTime 0.373 (0.404)\tLoss 0.0490 (0.0394)\t\n",
      "Epoch: [8] [295/385]\tTime 0.396 (0.403)\tLoss 0.0415 (0.0394)\t\n",
      "Epoch: [8] [300/385]\tTime 0.394 (0.403)\tLoss 0.0430 (0.0395)\t\n",
      "Epoch: [8] [305/385]\tTime 0.374 (0.403)\tLoss 0.0501 (0.0397)\t\n",
      "Epoch: [8] [310/385]\tTime 0.361 (0.403)\tLoss 0.0389 (0.0398)\t\n",
      "Epoch: [8] [315/385]\tTime 0.371 (0.403)\tLoss 0.0465 (0.0399)\t\n",
      "Epoch: [8] [320/385]\tTime 0.425 (0.403)\tLoss 0.0450 (0.0399)\t\n",
      "Epoch: [8] [325/385]\tTime 0.386 (0.403)\tLoss 0.0482 (0.0400)\t\n",
      "Epoch: [8] [330/385]\tTime 0.394 (0.403)\tLoss 0.0575 (0.0402)\t\n",
      "Epoch: [8] [335/385]\tTime 0.418 (0.403)\tLoss 0.0446 (0.0403)\t\n",
      "Epoch: [8] [340/385]\tTime 0.421 (0.403)\tLoss 0.0451 (0.0404)\t\n",
      "Epoch: [8] [345/385]\tTime 0.400 (0.403)\tLoss 0.0430 (0.0404)\t\n",
      "Epoch: [8] [350/385]\tTime 0.377 (0.403)\tLoss 0.0449 (0.0405)\t\n",
      "Epoch: [8] [355/385]\tTime 0.468 (0.403)\tLoss 0.0530 (0.0406)\t\n",
      "Epoch: [8] [360/385]\tTime 0.370 (0.403)\tLoss 0.0436 (0.0407)\t\n",
      "Epoch: [8] [365/385]\tTime 0.436 (0.403)\tLoss 0.0487 (0.0408)\t\n",
      "Epoch: [8] [370/385]\tTime 0.407 (0.403)\tLoss 0.0432 (0.0408)\t\n",
      "Epoch: [8] [375/385]\tTime 0.423 (0.404)\tLoss 0.0526 (0.0410)\t\n",
      "Epoch: [8] [380/385]\tTime 0.432 (0.404)\tLoss 0.0471 (0.0411)\t\n",
      "Saving Model...\n",
      "Model Saved Successfully!\n",
      "Epoch: [9] [0/385]\tTime 0.437 (0.437)\tLoss 0.0342 (0.0342)\t\n",
      "Saving Model...\n",
      "Model Saved Successfully!\n",
      "Epoch: [9] [5/385]\tTime 0.393 (0.415)\tLoss 0.0330 (0.0304)\t\n",
      "Epoch: [9] [10/385]\tTime 0.386 (0.412)\tLoss 0.0302 (0.0309)\t\n",
      "Epoch: [9] [15/385]\tTime 0.360 (0.407)\tLoss 0.0281 (0.0306)\t\n",
      "Epoch: [9] [20/385]\tTime 0.402 (0.401)\tLoss 0.0314 (0.0303)\t\n",
      "Epoch: [9] [25/385]\tTime 0.384 (0.396)\tLoss 0.0348 (0.0303)\t\n",
      "Epoch: [9] [30/385]\tTime 0.466 (0.400)\tLoss 0.0299 (0.0305)\t\n",
      "Epoch: [9] [35/385]\tTime 0.359 (0.398)\tLoss 0.0258 (0.0305)\t\n",
      "Epoch: [9] [40/385]\tTime 0.372 (0.398)\tLoss 0.0270 (0.0305)\t\n",
      "Epoch: [9] [45/385]\tTime 0.424 (0.397)\tLoss 0.0334 (0.0306)\t\n",
      "Epoch: [9] [50/385]\tTime 0.391 (0.396)\tLoss 0.0317 (0.0307)\t\n",
      "Epoch: [9] [55/385]\tTime 0.382 (0.394)\tLoss 0.0312 (0.0308)\t\n",
      "Epoch: [9] [60/385]\tTime 0.421 (0.398)\tLoss 0.0308 (0.0307)\t\n",
      "Epoch: [9] [65/385]\tTime 0.418 (0.398)\tLoss 0.0349 (0.0308)\t\n",
      "Epoch: [9] [70/385]\tTime 0.394 (0.398)\tLoss 0.0365 (0.0310)\t\n",
      "Epoch: [9] [75/385]\tTime 0.381 (0.397)\tLoss 0.0306 (0.0311)\t\n",
      "Epoch: [9] [80/385]\tTime 0.443 (0.399)\tLoss 0.0330 (0.0311)\t\n",
      "Epoch: [9] [85/385]\tTime 0.363 (0.399)\tLoss 0.0302 (0.0309)\t\n",
      "Epoch: [9] [90/385]\tTime 0.401 (0.398)\tLoss 0.0271 (0.0309)\t\n",
      "Epoch: [9] [95/385]\tTime 0.394 (0.400)\tLoss 0.0337 (0.0308)\t\n",
      "Epoch: [9] [100/385]\tTime 0.411 (0.400)\tLoss 0.0381 (0.0309)\t\n",
      "Epoch: [9] [105/385]\tTime 0.432 (0.403)\tLoss 0.0259 (0.0309)\t\n",
      "Epoch: [9] [110/385]\tTime 0.379 (0.403)\tLoss 0.0269 (0.0309)\t\n",
      "Epoch: [9] [115/385]\tTime 0.373 (0.403)\tLoss 0.0288 (0.0309)\t\n",
      "Epoch: [9] [120/385]\tTime 0.496 (0.404)\tLoss 0.0335 (0.0309)\t\n",
      "Epoch: [9] [125/385]\tTime 0.447 (0.404)\tLoss 0.0297 (0.0309)\t\n",
      "Epoch: [9] [130/385]\tTime 0.376 (0.403)\tLoss 0.0310 (0.0309)\t\n",
      "Epoch: [9] [135/385]\tTime 0.398 (0.403)\tLoss 0.0330 (0.0309)\t\n",
      "Epoch: [9] [140/385]\tTime 0.397 (0.402)\tLoss 0.0322 (0.0310)\t\n",
      "Epoch: [9] [145/385]\tTime 0.382 (0.402)\tLoss 0.0332 (0.0311)\t\n",
      "Epoch: [9] [150/385]\tTime 0.370 (0.402)\tLoss 0.0337 (0.0311)\t\n",
      "Epoch: [9] [155/385]\tTime 0.460 (0.402)\tLoss 0.0304 (0.0312)\t\n",
      "Epoch: [9] [160/385]\tTime 0.477 (0.403)\tLoss 0.0445 (0.0314)\t\n",
      "Epoch: [9] [165/385]\tTime 0.409 (0.403)\tLoss 0.0371 (0.0314)\t\n",
      "Epoch: [9] [170/385]\tTime 0.383 (0.402)\tLoss 0.0351 (0.0314)\t\n",
      "Epoch: [9] [175/385]\tTime 0.379 (0.402)\tLoss 0.0320 (0.0315)\t\n",
      "Epoch: [9] [180/385]\tTime 0.436 (0.402)\tLoss 0.0281 (0.0315)\t\n",
      "Epoch: [9] [185/385]\tTime 0.537 (0.403)\tLoss 0.0337 (0.0316)\t\n",
      "Epoch: [9] [190/385]\tTime 0.425 (0.403)\tLoss 0.0386 (0.0317)\t\n",
      "Epoch: [9] [195/385]\tTime 0.358 (0.403)\tLoss 0.0352 (0.0318)\t\n",
      "Epoch: [9] [200/385]\tTime 0.376 (0.403)\tLoss 0.0344 (0.0318)\t\n",
      "Saving Model...\n",
      "Model Saved Successfully!\n",
      "Epoch: [9] [205/385]\tTime 0.412 (0.403)\tLoss 0.0362 (0.0319)\t\n",
      "Epoch: [9] [210/385]\tTime 0.371 (0.402)\tLoss 0.0353 (0.0320)\t\n",
      "Epoch: [9] [215/385]\tTime 0.405 (0.403)\tLoss 0.0355 (0.0322)\t\n",
      "Epoch: [9] [220/385]\tTime 0.420 (0.402)\tLoss 0.0286 (0.0322)\t\n",
      "Epoch: [9] [225/385]\tTime 0.401 (0.402)\tLoss 0.0378 (0.0323)\t\n",
      "Epoch: [9] [230/385]\tTime 0.382 (0.402)\tLoss 0.0364 (0.0323)\t\n",
      "Epoch: [9] [235/385]\tTime 0.368 (0.401)\tLoss 0.0332 (0.0324)\t\n",
      "Epoch: [9] [240/385]\tTime 0.417 (0.401)\tLoss 0.0407 (0.0325)\t\n",
      "Epoch: [9] [245/385]\tTime 0.412 (0.402)\tLoss 0.0360 (0.0326)\t\n",
      "Epoch: [9] [250/385]\tTime 0.413 (0.402)\tLoss 0.0320 (0.0327)\t\n",
      "Epoch: [9] [255/385]\tTime 0.451 (0.403)\tLoss 0.0428 (0.0328)\t\n",
      "Epoch: [9] [260/385]\tTime 0.414 (0.404)\tLoss 0.0341 (0.0328)\t\n",
      "Epoch: [9] [265/385]\tTime 0.414 (0.404)\tLoss 0.0373 (0.0329)\t\n",
      "Epoch: [9] [270/385]\tTime 0.412 (0.405)\tLoss 0.0370 (0.0329)\t\n",
      "Epoch: [9] [275/385]\tTime 0.414 (0.404)\tLoss 0.0430 (0.0330)\t\n",
      "Epoch: [9] [280/385]\tTime 0.433 (0.404)\tLoss 0.0368 (0.0330)\t\n",
      "Epoch: [9] [285/385]\tTime 0.437 (0.405)\tLoss 0.0352 (0.0332)\t\n",
      "Epoch: [9] [290/385]\tTime 0.415 (0.405)\tLoss 0.0379 (0.0332)\t\n",
      "Epoch: [9] [295/385]\tTime 0.462 (0.405)\tLoss 0.0388 (0.0333)\t\n",
      "Epoch: [9] [300/385]\tTime 0.381 (0.406)\tLoss 0.0422 (0.0334)\t\n",
      "Epoch: [9] [305/385]\tTime 0.395 (0.406)\tLoss 0.0331 (0.0335)\t\n",
      "Epoch: [9] [310/385]\tTime 0.370 (0.406)\tLoss 0.0340 (0.0335)\t\n",
      "Epoch: [9] [315/385]\tTime 0.421 (0.406)\tLoss 0.0312 (0.0335)\t\n",
      "Epoch: [9] [320/385]\tTime 0.419 (0.406)\tLoss 0.0350 (0.0335)\t\n",
      "Epoch: [9] [325/385]\tTime 0.387 (0.406)\tLoss 0.0409 (0.0337)\t\n",
      "Epoch: [9] [330/385]\tTime 0.407 (0.406)\tLoss 0.0439 (0.0337)\t\n",
      "Epoch: [9] [335/385]\tTime 0.444 (0.406)\tLoss 0.0378 (0.0338)\t\n",
      "Epoch: [9] [340/385]\tTime 0.403 (0.406)\tLoss 0.0365 (0.0338)\t\n",
      "Epoch: [9] [345/385]\tTime 0.385 (0.406)\tLoss 0.0428 (0.0340)\t\n",
      "Epoch: [9] [350/385]\tTime 0.387 (0.406)\tLoss 0.0421 (0.0341)\t\n",
      "Epoch: [9] [355/385]\tTime 0.378 (0.406)\tLoss 0.0390 (0.0342)\t\n",
      "Epoch: [9] [360/385]\tTime 0.372 (0.405)\tLoss 0.0354 (0.0342)\t\n",
      "Epoch: [9] [365/385]\tTime 0.389 (0.405)\tLoss 0.0367 (0.0343)\t\n",
      "Epoch: [9] [370/385]\tTime 0.400 (0.406)\tLoss 0.0408 (0.0343)\t\n",
      "Epoch: [9] [375/385]\tTime 0.388 (0.406)\tLoss 0.0356 (0.0344)\t\n",
      "Epoch: [9] [380/385]\tTime 0.370 (0.405)\tLoss 0.0521 (0.0345)\t\n",
      "Saving Model...\n",
      "Model Saved Successfully!\n"
     ]
    }
   ],
   "source": [
    "# model train\n",
    "\n",
    "EPOCH = 10\n",
    "PRINT_FREQ = 5\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    train(encoder, decoder, epoch)\n",
    "    print('Saving Model...')\n",
    "    torch.save(encoder.state_dict(), 'eng2spa_encoder_params.pkl')\n",
    "    torch.save(decoder.state_dict(), 'eng2spa_decoder_params.pkl')\n",
    "    print('Model Saved Successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> ¿ podrias ensenarme el camino , por favor ? <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> \n",
      "<start> will you please show me the way ? <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> \n",
      "\n",
      "could could you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you \n",
      "<start> los prisioneros fugitivos siguen profugos . <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> \n",
      "<start> the escaped prisoners are still on the run . <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> \n",
      "\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the \n",
      "<start> los ninos a menudo hacen estupideces . <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> \n",
      "<start> children often do stupid things . <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> \n",
      "\n",
      "children children children children children children children children children children children children children children children children children children children children children children children children children children children children children children children children \n",
      "<start> ella intento no derramar ni una lagrima . <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> \n",
      "<start> she tried not to shed a tear . <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> \n",
      "\n",
      "she she not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not \n",
      "<start> ¿ a que es tremendo ? <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> \n",
      "<start> isn t that terrific ? <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/home/huagang/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    718\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/huagang/anaconda3/lib/python3.6/site-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    738\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 739\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    740\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/huagang/anaconda3/lib/python3.6/site-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    394\u001b[0m         \"\"\"\n\u001b[0;32m--> 395\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    396\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv (zmq/backend/cython/socket.c:7683)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv (zmq/backend/cython/socket.c:7460)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy (zmq/backend/cython/socket.c:2344)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/home/huagang/anaconda3/lib/python3.6/site-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc (zmq/backend/cython/socket.c:9621)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-7105ebc82298>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mdecoder_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'eng2spa_decoder_params.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-21-3f791e268539>\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(encoder, decoder)\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarg_lang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midx2word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_word1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0menc_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mdec_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menc_hidden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/huagang/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    692\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 694\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    695\u001b[0m         )\n\u001b[1;32m    696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/huagang/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    722\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 724\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    725\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "encoder_val = Encoder(vocab_inp_size, embedding_dim, units, 1)\n",
    "decoder_val = Decoder(vocab_tar_size, embedding_dim, units, units, 1)\n",
    "\n",
    "encoder_val.to(device)\n",
    "decoder_val.to(device)\n",
    "\n",
    "# print(encoder_val)\n",
    "\n",
    "encoder_val.load_state_dict(torch.load('eng2spa_encoder_params.pkl'))\n",
    "decoder_val.load_state_dict(torch.load('eng2spa_decoder_params.pkl'))\n",
    "\n",
    "validate(encoder_val, decoder_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
